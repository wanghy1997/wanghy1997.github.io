import{_ as l}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as d,b as r,a as n,e as a,d as o,f as i,w as s,r as p,o as h}from"./app-BSlDOcH0.js";const c={};function g(u,e){const t=p("RouteLink");return h(),d("div",null,[e[10]||(e[10]=r('<h1 id="readme🧐" tabindex="-1"><a class="header-anchor" href="#readme🧐"><span>ReadMe🧐</span></a></h1><h2 id="介绍" tabindex="-1"><a class="header-anchor" href="#介绍"><span>介绍</span></a></h2><p>该文档项目为 Happy Learning 小组的科研入门资料。<br> ReadMe仅包含基础内容，可作为索引使用，详细内容请见对应文档。<br> 项目地址: <a href="http://10.16.104.13:1805/happy-learning/tohappylearning" target="_blank" rel="noopener noreferrer">http://10.16.104.13:1805/happy-learning/tohappylearning</a></p><h2 id="小组老大" tabindex="-1"><a class="header-anchor" href="#小组老大"><span>小组老大</span></a></h2><p>冷佳旭 <a href="https://faculty.cqupt.edu.cn/lengjiaxu/zh_CN/index.htm" target="_blank" rel="noopener noreferrer">https://faculty.cqupt.edu.cn/lengjiaxu/zh_CN/index.htm</a></p><h2 id="使用指南" tabindex="-1"><a class="header-anchor" href="#使用指南"><span>使用指南</span></a></h2><p>平台使用指南请见 <a href="http://10.16.104.13:1805/root/users-guide" target="_blank" rel="noopener noreferrer">http://10.16.104.13:1805/root/users-guide</a></p><p>此 <code>README</code> 文档仅作为索引, 主要内容为入门基础知识和各研究方向简介，详细笔记请见 <code>1.科研方向</code> 文件夹。</p><hr><h2 id="文档总览" tabindex="-1"><a class="header-anchor" href="#文档总览"><span>文档总览</span></a></h2><p>[toc]</p><hr><h2 id="新生入门" tabindex="-1"><a class="header-anchor" href="#新生入门"><span>新生入门</span></a></h2>',13)),n("ol",null,[e[2]||(e[2]=n("li",null,"首先学习 Andrew Ng 的 Machine Learning 在线课程，要做课后作业(MATLAB)；",-1)),e[3]||(e[3]=n("li",null,"其次学习 Andrew Ng 的 Deep Learning 在线课程，要做课后作业（Python+tensorflow），这个过程可能需要花3-5天学习python的基础语法；",-1)),e[4]||(e[4]=n("li",null,"学习CS231n在线课程，并完成大作业。",-1)),n("li",null,[e[1]||(e[1]=o("深度学习基础练习及经典模型 ",-1)),i(t,{to:"/browser/learning/3.%E5%85%A5%E9%97%A8%E5%AE%9E%E8%B7%B5/Torch_learning/"},{default:s(()=>[...e[0]||(e[0]=[o("链接入口🔗, 点击进入🔗",-1)])]),_:1})]),e[5]||(e[5]=n("li",null,[n("a",{href:"https://mofanpy.com/",target:"_blank",rel:"noopener noreferrer"},"https://mofanpy.com/")],-1))]),e[11]||(e[11]=r('<h2 id="科研基础" tabindex="-1"><a class="header-anchor" href="#科研基础"><span>科研基础</span></a></h2><ol><li>基础课程：本硕期间重点掌握 <code>《高等数学》</code> <code>《线性代数》</code> <code>《概率论与数理统计》</code> <code>《矩阵论》</code> <code>《凸优化》</code> 课程，这里的掌握是要理解它们的本质、作用及物理几何意义，而非应付考试。在学习时抛弃国内出版的死板教材，搜索国内外优秀书籍、公开课、视频进行学习。<br> 例如，《线性代数》推荐<a href="https://www.bilibili.com/video/BV1ys411472E" target="_blank" rel="noopener noreferrer">《线性代数的本质》系列视频</a><a href="https://www.bilibili.com/video/BV1ys411472E" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1ys411472E</a>，以及Gilbert Strang 的麻省理工大学公开课视频<a href="https://www.bilibili.com/video/BV1zx411g7gq" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1zx411g7gq</a>。</li><li>机器学习：推荐南京大学周志华老师的西瓜书《机器学习》，华为诺亚方舟实验室主任李航老师的《统计学习方法》，斯坦福大学Andrew NG（吴恩达）老师的机器学习课程视频<a href="https://www.bilibili.com/video/BV164411b7dx" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV164411b7dx</a>。</li><li>深度学习：推荐Google研究科学家Ian Goodfellow（Generative Adversarial Nets提出者）的花书《Deep Learning》（有中文译本人民邮电出版社），斯坦福大学李飞飞（ImageNet的提出者）老师的cs231n计算机视觉课程<a href="https://www.bilibili.com/video/BV1nJ411z7fe" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1nJ411z7fe</a>。</li><li>科研环境：操作系统Linux，主要编程语言Python，主要深度学习框架Pytorch。因此，要掌握Linux的使用方法与常用命令，熟练使用Python语言编程，熟练使用Pytorch深度学习框架搭建网络与深度学习算法。<br> a. Linux 学习：<a href="https://www.runoob.com/linux/linux-tutorial.html" target="_blank" rel="noopener noreferrer">https://www.runoob.com/linux/linux-tutorial.html</a>；<br> b. Python 学习：<a href="https://www.runoob.com/python3/python3-tutorial.html" target="_blank" rel="noopener noreferrer">https://www.runoob.com/python3/python3-tutorial.html</a>；<br> c. Python 第三方库 Pillow（处理图像）学习：<a href="https://pillow-cn.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener noreferrer">https://pillow-cn.readthedocs.io/zh_CN/latest/</a>；<br> d. Python 第三方库 Numpy（CPU处理多维张量）学习：<a href="https://www.runoob.com/numpy/numpy-tutorial.html" target="_blank" rel="noopener noreferrer">https://www.runoob.com/numpy/numpy-tutorial.html</a>；<br> e. 深度学习框架Pytorch（使用Python编程，GPU处理多维张量，提供深度学习API）学习：<a href="https://pytorch.apachecn.org/docs/1.4/" target="_blank" rel="noopener noreferrer">https://pytorch.apachecn.org/docs/1.4/</a>。</li></ol><h2 id="科研工具" tabindex="-1"><a class="header-anchor" href="#科研工具"><span>科研工具</span></a></h2>',3)),n("ol",null,[e[9]||(e[9]=r('<li>科学上网：一个可用稳定的上谷歌（翻墙）工具是必须的，推荐：<a href="https://geckoiplc.org/register?aff=nxkYWh33" target="_blank" rel="noopener noreferrer">https://geckoiplc.org/register?aff=nxkYWh33</a>，使用方法该网站有教程</li><li>检索论文：GoogleScholar：<a href="https://scholar.google.com/" target="_blank" rel="noopener noreferrer">https://scholar.google.com/</a>（需翻墙）</li><li>下载论文：<br> a. IEEE：<a href="https://ieeexplore.ieee.org/Xplore/home.jsp" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/Xplore/home.jsp</a>；<br> b. ScienceDirect：<a href="https://www.sciencedirect.com/" target="_blank" rel="noopener noreferrer">https://www.sciencedirect.com/</a>；<br> c. SpringerLink：<a href="https://link.springer.com/" target="_blank" rel="noopener noreferrer">https://link.springer.com/</a>；<br> d. arXiv：<a href="https://arxiv.org/" target="_blank" rel="noopener noreferrer">https://arxiv.org/</a>占坑网站，最新的文章会发表在上面占坑，但是良莠不齐，免费下载，翻墙会快；<br> e. Sci-Hub：<a href="https://www.sci-hub.ren/" target="_blank" rel="noopener noreferrer">https://www.sci-hub.ren/</a>不在学校内又想下载IEEE等版权收费文章？Sci-Hub是神器，网站宣言...to remove all the barriers in the way of science，地址可能会变，因为一直被告。</li><li>管理论文：Mendely：<a href="https://www.mendeley.com/" target="_blank" rel="noopener noreferrer">https://www.mendeley.com/</a>；Endnotes：<a href="https://www.endnote.com/" target="_blank" rel="noopener noreferrer">https://www.endnote.com/</a>；Zotero：<a href="https://www.zotero.org/" target="_blank" rel="noopener noreferrer">https://www.zotero.org/</a>；自己选择。</li><li>撰写论文：LaTex环境安装：<a href="http://tug.org/texlive/" target="_blank" rel="noopener noreferrer">http://tug.org/texlive/</a>；按着网站指导来。也可以使用在线的LaTex编辑工具如Overleaf：<a href="https://www.overleaf.com/login" target="_blank" rel="noopener noreferrer">https://www.overleaf.com/login</a>。</li><li>论文作图：PPT、Visio(Windows)、OmniGraffle(Mac)；<br> 直接保存成PDF就是矢量图，使用Adobe Acrobat将图片多余部分裁剪。</li><li>代码编辑：VSCode（Visual Studio Code）：<a href="https://code.visualstudio.com/" target="_blank" rel="noopener noreferrer">https://code.visualstudio.com/</a>；<br> 你想要的代码编辑功能都能通过VSCode+插件实现。<br> 常用VSCode插件：LaTex Workshop（用VSCode写论文，前提得先装好LaTex环境）、Remote-SSH（用VSCode直接浏览编辑远程服务器上的代码和图片）、One Dark Pro（让VSCode更好看）、Bracket Pair Colorizer 2（让代码每一级括号带不同的颜色，括号再也不会不成对报错了）、Code Spell Checker（检查你的拼写是否有错误）、indent-rainbow（不同级的缩进带不同颜色，对Python来说很有用）、Python（可以直接在VSCode调试Python程序，前提得先装好Python环境）。</li>',7)),n("li",null,[e[7]||(e[7]=o("服务器后台运行管理工具tmux ",-1)),i(t,{to:"/browser/learning/2.%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7tmux.html"},{default:s(()=>[...e[6]||(e[6]=[o("链接入口🔗, 点击进入🔗",-1)])]),_:1}),e[8]||(e[8]=o("。",-1))])]),e[12]||(e[12]=r('<h2 id="科研方向" tabindex="-1"><a class="header-anchor" href="#科研方向"><span>科研方向</span></a></h2><h3 id="✧-行人检测-person-detection-高峰-博士" tabindex="-1"><a class="header-anchor" href="#✧-行人检测-person-detection-高峰-博士"><span>✧ 行人检测（Person Detection）-&gt; <code>高峰 (博士)</code></span></a></h3><h4 id="论文汇总-论文汇总链接-例如github上的-awesome-系列" tabindex="-1"><a class="header-anchor" href="#论文汇总-论文汇总链接-例如github上的-awesome-系列"><span>论文汇总：论文汇总链接，例如GitHub上的 <code>Awesome</code> 系列</span></a></h4><h5 id="➢-推荐综述" tabindex="-1"><a class="header-anchor" href="#➢-推荐综述"><span>➢ 推荐综述</span></a></h5><p>[1] From Handcrafted to Deep Features for Pedestrian Detection: A Survey (TPAMI2021) 涵盖了2020年及以前从传统到深度学习的行人检测所有知识。但是，2020Detr横空出世,2021突飞猛进，2022基于Detr思想的第一篇行人检测顶会问世,个人觉得这是里程碑式的作品。因为自Faster-RCNN问世以来，行人检测领域百分之80以上的文章都是Faster-RCNN的变种。未来可能Detr替代Faster-RCNN，成为行人检测领域的Baseline。目前DETR相关资料非常少，这部分知识需要自己想办法。</p><h5 id="➢-优秀团队-学术大佬" tabindex="-1"><a class="header-anchor" href="#➢-优秀团队-学术大佬"><span>➢ 优秀团队 / 学术大佬</span></a></h5><p>■ 【旷视团队】<br> [1] Detection in Crowded Scenes: One Proposal, Multiple Predictions. CVPR2020<br> [2] End-to-End Object Detection with Fully Convolutional Network. CVPR2021<br> [3] Progressive End-to-End Object Detection in Crowded Scenes. CVPR2022<br> ■ 【Peize Sun】【香港大学在读博士】<br> [1] Sparse r-cnn: End-to-end object detection with learnable proposals. CVPR2021<br> [2] What Makes for End-to-End Object Detection? ICML2021</p><h5 id="➢-经典项目" tabindex="-1"><a class="header-anchor" href="#➢-经典项目"><span>➢ 经典项目</span></a></h5><p>○ 上述五篇论文都开源了代码，地址在论文中。第一篇论文开源的代码中包含了纯pytorch实现的Faster-RCNN和Retinanet非常适合初学者入门基于CNN的目标检测算法。<br> ○ 我上传了一个简易版的DETR纯pytorch实现在我的科研方向文件夹中，可以很好的学习DETR原理。<br> ○ 旷视团队的第三篇论文是Peize Sun第一篇论文的改进，通过阅读代码可以从一个角度了解目前研究者改进Detr算法的思路。</p><h3 id="✧-行人重识别-person-re-identification-汪海涛" tabindex="-1"><a class="header-anchor" href="#✧-行人重识别-person-re-identification-汪海涛"><span>✧ 行人重识别（Person Re-identification）-&gt; <code>汪海涛</code></span></a></h3><h4 id="论文汇总-https-github-com-bismex-awesome-person-re-identification-该-repo-内有目前-video-reid-方向的顶会所有论文汇总-包括基本分类、-常用数据库下载、常用-code" tabindex="-1"><a class="header-anchor" href="#论文汇总-https-github-com-bismex-awesome-person-re-identification-该-repo-内有目前-video-reid-方向的顶会所有论文汇总-包括基本分类、-常用数据库下载、常用-code"><span>论文汇总: <a href="https://github.com/bismex/Awesome-person-re-identification" target="_blank" rel="noopener noreferrer">https://github.com/bismex/Awesome-person-re-identification</a>，该 repo 内有目前 video reid 方向的顶会所有论文汇总，包括基本分类、 常用数据库下载、常用 code</span></a></h4><h5 id="➢-推荐综述-1" tabindex="-1"><a class="header-anchor" href="#➢-推荐综述-1"><span>➢ 推荐综述</span></a></h5><p>[1] Deep Learning for Person Re-identification: A Survey and Outlook, TPAMI 2020(近期的 reid 工作汇总)<br> [2] Person Re-identification: Past, Present and Future, arXiv 2016(早期的 reid 工作汇总)</p><ul><li>基本看上上面两个 survey 对目前 reid 的发展脉络就可以有一个清晰的把握了</li><li>看论文的时候 related work 里面的论文都可以参考看一看</li><li>一些必备的技术点:bagtricks, agw, sbs, pcb, mgn, arcface</li></ul><h5 id="➢-优秀团队-学术大佬-1" tabindex="-1"><a class="header-anchor" href="#➢-优秀团队-学术大佬-1"><span>➢ 优秀团队 / 学术大佬</span></a></h5><p>■ 郑伟诗 Wei-Shi Zheng 中山大学智能科学与系统实验室(iSEE) <a href="https://www.isee-ai.cn/~zhwshi/" target="_blank" rel="noopener noreferrer">https://www.isee-ai.cn/~zhwshi/</a><br> [1] J. Yang, W. -S. Zheng, Q. Yang, Y. -C. Chen and Q. Tian, &quot;Spatial-Temporal Graph Convolutional Network for Video-Based Person Re-Identification,&quot; 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 3286-3296, doi: 10.1109/CVPR42600.2020.00335.<br> [2] J. Yang et al., &quot;Learning to Know Where to See: A Visibility-Aware Approach for Occluded Person Re-identification,&quot; 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 11865-11874, doi: 10.1109/ICCV48922.2021.01167.</p><p>■ 常虹 Chang Hong 中国科学院计算机技术研究院(CAS) <a href="https://people.ucas.ac.cn/~changhong" target="_blank" rel="noopener noreferrer">https://people.ucas.ac.cn/~changhong</a><br> [1] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan and X. Chen, &quot;VRSTC: Occlusion-Free Video Person Re-Identification,&quot; 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 7176-7185, doi: 10.1109/CVPR.2019.00735.<br> [2] R. Hou, H. Chang, B. Ma, R. Huang and S. Shan, &quot;BiCnet-TKS: Learning Efficient Spatial-Temporal Representation for Video Person Re-Identification,&quot; 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 2014-2023, doi: 10.1109/CVPR46437.2021.00205.</p><h5 id="➢-可见光-reid-代表论文" tabindex="-1"><a class="header-anchor" href="#➢-可见光-reid-代表论文"><span>➢ 可见光 reid 代表论文</span></a></h5><p>[1] Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification, ECCV2020<br> [2] Identity-Guided Human Semantic Parsing for Person Re-Identification, ECCV2020<br> [3] Faster Person Re-Identification, ECCV20 (轻量级 reid， <a href="https://github.com/wangguanan/light-reid" target="_blank" rel="noopener noreferrer">https://github.com/wangguanan/light-reid</a> )<br> [4] Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive Person Re-Identification, ECCV2020<br> [5] Re-Ranking Person Re-Identification With k-Reciprocal Encoding， CVPR2017(将 reranking 应用到 reid，涨点显著)<br> [6] Person Re-Identification in the Wild, CVPR2017(从检测到识别)</p><h5 id="➢-异质-reid-代表论文" tabindex="-1"><a class="header-anchor" href="#➢-异质-reid-代表论文"><span>➢ 异质 reid 代表论文</span></a></h5><p>[1] Hi-CMD: Hierarchical Cross-Modality Disentanglement for Visible-Infrared Person Re-Identification，CVPR2020<br> [2] Cross-Modality Person Re-Identification With Shared-Specific Feature Transfer, CVPR2020<br> [3] RGB-Infrared Cross-Modality Person Re-Identification via Joint Pixel and Feature Alignment, ICCV2019<br> [4] Infrared-Visible Cross-Modal Person Re-Identification with an X Modality, AAAI2020</p><h5 id="➢-常用开源项目" tabindex="-1"><a class="header-anchor" href="#➢-常用开源项目"><span>➢ 常用开源项目</span></a></h5><ul><li>strong baseline: <a href="https://github.com/michuanhaohao/reid-strong-baseline" target="_blank" rel="noopener noreferrer">https://github.com/michuanhaohao/reid-strong-baseline</a></li><li>FAST-REID: <a href="https://github.com/JDAI-CV/fast-reid" target="_blank" rel="noopener noreferrer">https://github.com/JDAI-CV/fast-reid</a></li></ul><h3 id="✧-超分辨率重建-super-resolution" tabindex="-1"><a class="header-anchor" href="#✧-超分辨率重建-super-resolution"><span>✧ 超分辨率重建（Super-resolution）</span></a></h3>',24)),a("模版"),e[13]||(e[13]=r('<h4 id="论文汇总-论文汇总链接-例如github上的-awesome-系列-1" tabindex="-1"><a class="header-anchor" href="#论文汇总-论文汇总链接-例如github上的-awesome-系列-1"><span>论文汇总：论文汇总链接，例如GitHub上的 <code>Awesome</code> 系列</span></a></h4><h5 id="➢-推荐综述-2" tabindex="-1"><a class="header-anchor" href="#➢-推荐综述-2"><span>➢ 推荐综述</span></a></h5><p>[1] Video Super Resolution Based on Deep Learning: A comprehensive survey<br> [2] Deep Learning for Image Super-resolution: A Survey<br> [3] Blind Image Super-Resolution: A Survey and Beyond</p><ul><li>三篇综述分别代表超分领域中的三个小领域：视频超分辨率、单图超分辨率、盲图像超分辨率。</li></ul><h5 id="➢-优秀团队-学术大佬-2" tabindex="-1"><a class="header-anchor" href="#➢-优秀团队-学术大佬-2"><span>➢ 优秀团队 / 学术大佬</span></a></h5><p>■ Xintao Wang:现任腾讯ARC实验室（深圳）研究员。毕业于香港中文大学多媒体实验室。主要研究图像和视频的恢复。<br> 个人主页：<a href="https://xinntao.github.io/" target="_blank" rel="noopener noreferrer">https://xinntao.github.io/</a><br> [1] Wang X, Chan K C K, Yu K, et al. Edvr: Video restoration with enhanced deformable convolutional networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2019: 0-0.<br> [2] Wang X, Yu K, Dong C, et al. Recovering realistic texture in image super-resolution by deep spatial feature transform[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 606-615.<br> [3] Wang X, Yu K, Wu S, et al. Esrgan: Enhanced super-resolution generative adversarial networks[C]//Proceedings of the European conference on computer vision (ECCV) workshops. 2018: 0-0.<br> ■ Kelvin C.K. Chan：新加坡南洋理工大学，计算机科学与工程学院，香港中文大学学士学位。<br> 个人主页：<a href="https://ckkelvinchan.github.io/" target="_blank" rel="noopener noreferrer">https://ckkelvinchan.github.io/</a><br> [1] Chan K C K, Wang X, Yu K, et al. BasicVSR: The search for essential components in video super-resolution and beyond[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 4947-4956.<br> [2] Chan K C K, Zhou S, Xu X, et al. BasicVSR++: Improving video super-resolution with enhanced propagation and alignment[J]. arXiv preprint arXiv:2104.13371, 2021.<br> [3] Chan K C K, Wang X, Yu K, et al. Understanding deformable alignment in video super-resolution[J]. arXiv preprint arXiv:2009.07265, 2020, 4(3): 4.</p><h5 id="➢-经典论文-推荐加-👍" tabindex="-1"><a class="header-anchor" href="#➢-经典论文-推荐加-👍"><span>➢ 经典论文：（推荐加“👍”）</span></a></h5><ul><li><p>Video Super-Resolution<br> [1] Ward C M, Harguess J, Crabb B, et al. Image quality assessment for determining efficacy and limitations of Super-Resolution Convolutional Neural Network (SRCNN)[C]//Applications of Digital Image Processing XL. International Society for Optics and Photonics, 2017, 10396: 1039605.<br> 简介：该篇论文是深度学习在超分辨率的开山之作。每个入门SR的学者都需要从这一篇论文开始。<br> [2] Liu D, Wang Z, Fan Y, et al. Robust video super-resolution with learned temporal dynamics[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 2507-2515.<br> [3] Tian Y, Zhang Y, Fu Y, et al. Tdan: Temporally-deformable alignment network for video super-resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3360-3369.<br> [4] 👍【Wang X, Chan K C K, Yu K, et al. Edvr: Video restoration with enhanced deformable convolutional networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2019: 0-0.<br> [5] Haris M, Shakhnarovich G, Ukita N. Recurrent back-projection network for video super-resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 3897-3906.<br> [6] Chan K C K, Wang X, Yu K, et al. BasicVSR: The search for essential components in video super-resolution and beyond[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 4947-4956.<br> [7] Chan K C K, Wang X, Yu K, et al. Understanding deformable alignment in video super-resolution[J]. arXiv preprint arXiv:2009.07265, 2020, 4(3): 4.</p></li><li><p>Blind Image Super-Resolution<br> [1] Zhang K, Zuo W, Zhang L. Learning a single convolutional super-resolution network for multiple degradations[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 3262-3271.<br> [2] Gu J, Lu H, Zuo W, et al. Blind super-resolution with iterative kernel correction[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 1604-1613.<br> [3] Wei Y, Gu S, Li Y, et al. Unsupervised real-world image super resolution via domain-distance aware training[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 13385-13394.<br> [4] Shocher A, Cohen N, Irani M. “zero-shot” super-resolution using deep internal learning[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 3118-3126.<br> [5] Hui Z, Li J, Wang X, et al. Learning the Non-differentiable Optimization for Blind Super-Resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 2093-2102.</p></li></ul><h5 id="➢-经典开源项目" tabindex="-1"><a class="header-anchor" href="#➢-经典开源项目"><span>➢ 经典开源项目</span></a></h5><p>○ 【名称】【链接】【简介】</p><p>[1] BasicVSR: <a href="https://github.com/xinntao/BasicSR" target="_blank" rel="noopener noreferrer">https://github.com/xinntao/BasicSR</a><br> 简介：该项目里包含了很多经典论文的复现，因此可直接将这个开源代码用会就可。</p><h3 id="✧-视频异常检测-video-anomaly-detection-谭明圮" tabindex="-1"><a class="header-anchor" href="#✧-视频异常检测-video-anomaly-detection-谭明圮"><span>✧ 视频异常检测 (Video Anomaly Detection) -&gt; <code>谭明圮</code></span></a></h3>',12)),a("模版"),e[14]||(e[14]=r('<h4 id="论文汇总" tabindex="-1"><a class="header-anchor" href="#论文汇总"><span>论文汇总：</span></a></h4><p>[1] <a href="https://github.com/fjchange/awesome-video-anomaly-detection" target="_blank" rel="noopener noreferrer">https://github.com/fjchange/awesome-video-anomaly-detection</a> 该 repo 内有目前 视频异常检测（VAD） 方向的优秀论文汇总，包括基本分类、 常用数据库下载、 开源code、 综述<br> [2] <a href="https://github.com/shot1107/anomaly_detection_papers" target="_blank" rel="noopener noreferrer">https://github.com/shot1107/anomaly_detection_papers</a> 该repo 内有异常检测每年顶会的论文，包括但不限于视频异常检测，可参考借鉴。</p><h5 id="➢-认识异常检测" tabindex="-1"><a class="header-anchor" href="#➢-认识异常检测"><span>➢ 认识异常检测</span></a></h5><h6 id="_1-简单介绍-从异常行为检测-视频异常行为检测" tabindex="-1"><a class="header-anchor" href="#_1-简单介绍-从异常行为检测-视频异常行为检测"><span>1. 简单介绍（从异常行为检测--&gt; 视频异常行为检测）</span></a></h6><p>[1] 异常行为检测简介： <a href="https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw</a><br> [2] 基于深度学习的异常行为检测介绍：<a href="https://mp.weixin.qq.com/s/Aghbz4m1eWFCNGgEy8q6Cg" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/Aghbz4m1eWFCNGgEy8q6Cg</a><br> [3] 基于深度学习的异常行为检测研究现状： <a href="https://mp.weixin.qq.com/s/MwpELRlC1cuDgqn4staAzA" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/MwpELRlC1cuDgqn4staAzA</a><br> [4] 基于深度学习的视频异常行为事件检测简介: <a href="https://mp.weixin.qq.com/s/i3Xw2-ivARnF7rBSFtxugw" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/i3Xw2-ivARnF7rBSFtxugw</a><br> [5] 基于视频的异常行为检测算法介绍: <a href="https://mp.weixin.qq.com/s/Dxsc3oCuO0wYkeFubMfSNw" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/Dxsc3oCuO0wYkeFubMfSNw</a></p><h6 id="_2-论文综述" tabindex="-1"><a class="header-anchor" href="#_2-论文综述"><span>2.论文综述：</span></a></h6><p>[1] 邬开俊等. 视频异常检测技术研究进展[J]. 计算机科学与探索, 2022 （中文综述，但没有那么全面，可以有一个初步了解）<br> [2] Bharathkumar Ramachandra et al. A survey of single-scene video anomaly detection (TPAMI 2020)</p><h5 id="➢-优秀团队-学术大佬-3" tabindex="-1"><a class="header-anchor" href="#➢-优秀团队-学术大佬-3"><span>➢ 优秀团队 / 学术大佬</span></a></h5><h6 id="■-高盛华-上海科技大学-视觉与数据智能中心" tabindex="-1"><a class="header-anchor" href="#■-高盛华-上海科技大学-视觉与数据智能中心"><span>■ 高盛华 上海科技大学（视觉与数据智能中心）</span></a></h6><p>[1] A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework <strong>(ICCV 2017)</strong> --&gt;proposed Shanghaitech dataset.<br> [2] Future Frame Prediction for Anomaly Detection – A New Baseline <strong>(CVPR 2018)</strong><br> [3] Future Frame Prediction for Anomaly Detection <strong>(TPAMI 2022)</strong></p><h6 id="■-radu-ionescu-securifai-university-of-bucharest" tabindex="-1"><a class="header-anchor" href="#■-radu-ionescu-securifai-university-of-bucharest"><span>■ Radu Ionescu SecurifAI/University of Bucharest</span></a></h6><p>[1] Detecting abnormal events in video using Narrowed Normality Clusters <strong>(WACV 2019)</strong><br> [2] Object-centric Auto-encoders and Dummy Anomalies for Abnormal Event Detection in Video <strong>(CVPR 2019)</strong><br> [3] Anomaly Detection in Video via Self-Supervised and Multi-Task Learning <strong>(CVPR 2021)</strong><br> [4] A Background-Agnostic Framework with Adversarial Training for Abnormal Event Detection in Video <strong>(TPAMI 2021)</strong><br> [5] UBnormal New Benchmark for Supervised Open-Set Video Anomaly Detection <strong>(CVPR 2022)</strong><br> [6] Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection <strong>(CVPR 2022)</strong></p><h5 id="➢-经典论文-推荐加-👍-1" tabindex="-1"><a class="header-anchor" href="#➢-经典论文-推荐加-👍-1"><span>➢ 经典论文：（推荐加“👍”）</span></a></h5><h6 id="■-unsupervised-vad" tabindex="-1"><a class="header-anchor" href="#■-unsupervised-vad"><span>■ Unsupervised VAD</span></a></h6><ul><li><p><strong>Conference Papers</strong><br> [1] Learning Temporal Regularity in Video Sequences <strong>(CVPR 2016)</strong><br> [2] A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework --&gt;<strong>Proposed Shanghaitech dataset.</strong><br> [2] 👍Future Frame Prediction for Anomaly Detection -- A New Baseline <strong>(CVPR 2018)</strong><br> [3] 👍Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection <strong>(ICCV 2019)</strong> --&gt; <strong>The first to employ memory module on video anomaly detection</strong><br> [4] 👍Object-Centric Auto-Encoders and Dummy Anomalies for Abnormal Event Detection <strong>(CVPR 2019)</strong> --&gt; <strong>The first to combine object detection and vad to achieve object-level anomaly dtection.</strong><br> [5] AnoPCN: Video Anomaly Detection via Deep Predictive Coding Network <strong>(ACM MM 2019)</strong> --&gt; <strong>The first hybrid model</strong><br> [6] 👍Learning Memory-guided Normality for Anomaly Detection <strong>(CVPR 2020)</strong> --&gt; <strong>Based on MemAE</strong><br> [7] Cluster Attention Contrast for Video Anomaly Detection <strong>(ACM MM 2020)</strong> --&gt; <strong>The first to apply Contrastive Learninig</strong><br> [8] 👍Anomaly Detection in Video via Self-Supervised and Multi-Task Learning <strong>(CVPR 2021)</strong> --&gt; <strong>object-level</strong><br> [9] 👍A Hybrid Video Anomaly Detection Framework via Memory-Augmented Flow Reconstruction and Flow-Guided Frame Prediction <strong>(ICCV 2021)</strong> --&gt; <strong>Hybrid model</strong><br> [10] Anomaly Detection in Video Sequence with Appearance-Motion Correspondence (ICCV 2019) --&gt; <strong>Two stream network</strong><br> [11] Video Anomaly Detection and Localization via Gaussian Mixture Fully Convolutional Variational Autoencoder --&gt; <strong>Two stream network</strong><br> [12] Self-supervised Sparse Representation for Video Anomaly Detection <strong>(ECCV 2022)</strong> --&gt; A first attempt to slove unsupervised and weakly supervised VAD<br> [13] Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles <strong>(ECCV 2022)</strong></p></li><li><p><strong>Joural Papers</strong><br> [1] Video Anomaly Detection with Sparse Coding Inspired Deep Neural Networks <strong>(TPAMI 2021)</strong><br> [2] A Background-Agnostic Framework With Adversarial Training for Abnormal Event Detection in Video <strong>(TPAMI 2022)</strong><br> [3] Influence-Aware Attention Networks for Anomaly Detection in Surveillance Videos <strong>(TCSVT 2022)</strong><br> [4] Bidirectional Spatio-Temporal Feature Learning With Multiscale Evaluation for Video Anomaly Detection <strong>(TCSVT 2022)</strong><br> [5] Anomaly Detection With Bidirectional Consistency in Videos <strong>(TNNLS 2022)</strong><br> [6] Variational Abnormal Behavior Detection With Motion Consistency <strong>(TIP 2022)</strong><br> [7] DoTA: Unsupervised Detection of Traffic Anomaly in Driving Videos <strong>(TPAMI 2023)</strong><br> [8] A Hierarchical Spatio-Temporal Graph Convolutional Neural Network for Anomaly Detection in Videos <strong>(TCSVT 2023)</strong><br> [9] Learnable Locality-Sensitive Hashing for Video Anomaly Detection <strong>(TCSVT 2023)</strong><br> [10] A Kalman Variational Autoencoder Model Assisted by Odometric Clustering for Video Frame Prediction and Anomaly Detection <strong>(TIP 2023)</strong><br> [11] Abnormal Event Detection and Localization via Adversarial Event Prediction <strong>(TNNLS 2023)</strong></p></li></ul><h6 id="■-weakly-supervised-vad" tabindex="-1"><a class="header-anchor" href="#■-weakly-supervised-vad"><span>■ Weakly supervised VAD</span></a></h6><p>[1] 👍 Real-world Anomaly Detection in Surveillance Videos <strong>(CVPR 2018)</strong><br> [2] Weakly Supervised Video Anomaly Detection via Center-Guided Discrimative Learning <strong>(ICME 2020)</strong></p><p>[3] Decouple and Resolve: Transformer-Based Models for Online Anomaly Detection From Weakly Labeled Videos <strong>(TIFS 2023)</strong></p><h5 id="➢-经典项目-1" tabindex="-1"><a class="header-anchor" href="#➢-经典项目-1"><span>➢ 经典项目</span></a></h5><p>○ MNAD --&gt; <a href="https://github.com/cvlab-yonsei/MNAD" target="_blank" rel="noopener noreferrer">https://github.com/cvlab-yonsei/MNAD</a> 可作为baseline.</p><h5 id="➢-发现的新的有意思的研究方向-explainable-anomaly-detection-ead-可解释性异常检测" tabindex="-1"><a class="header-anchor" href="#➢-发现的新的有意思的研究方向-explainable-anomaly-detection-ead-可解释性异常检测"><span>➢ 发现的新的有意思的研究方向--&gt; Explainable Anomaly Detection (EAD) 可解释性异常检测</span></a></h5><h6 id="_1-definition" tabindex="-1"><a class="header-anchor" href="#_1-definition"><span>1. DEFINITION</span></a></h6><p>The aim of this TASK is to detect and automatically generate high-level explanations of anomalous events in video. Understanding the cause of an anomalous event is crucialas the required response is dependant on its nature andseverity. --&gt; Anomaly Detection &amp; Anoamly Explanation</p><h6 id="_2-related-work" tabindex="-1"><a class="header-anchor" href="#_2-related-work"><span>2. RELATED WORK</span></a></h6><p>[1] Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge (ICCV 2017)<br> [2] X-MAN: Explaining multiple sources of anomalies in video (CVPR workshop 2021)<br> [3] Discrete neural representations for explainable anomaly detection (WACV 2022)</p><h3 id="✧-航拍图像目标检测-drone-view-object-detection-莫梦竟成" tabindex="-1"><a class="header-anchor" href="#✧-航拍图像目标检测-drone-view-object-detection-莫梦竟成"><span>✧ 航拍图像目标检测（Drone-view Object Detection）-&gt; <code>莫梦竟成</code></span></a></h3>',26)),a("模版"),e[15]||(e[15]=r('<h4 id="论文汇总-论文汇总链接-例如github上的-awesome-系列-2" tabindex="-1"><a class="header-anchor" href="#论文汇总-论文汇总链接-例如github上的-awesome-系列-2"><span>论文汇总：论文汇总链接，例如GitHub上的 <code>Awesome</code> 系列</span></a></h4><h5 id="➢-推荐综述-3" tabindex="-1"><a class="header-anchor" href="#➢-推荐综述-3"><span>➢ 推荐综述</span></a></h5><p>[1] 【发表于】【年份】【论文名】【团队名】【论文链接】【代码链接】【简介】<br> [2] 【知乎/微信/博客等网页链接】【内容简介】</p><h5 id="➢-优秀团队-学术大佬-4" tabindex="-1"><a class="header-anchor" href="#➢-优秀团队-学术大佬-4"><span>➢ 优秀团队 / 学术大佬</span></a></h5><p>■ 【团队名】【团队链接】【所属机构】【细化方向】<br> [1] 【代表论文】<br> ■ 【大佬名】【个人主页】【所属机构】【细化方向】<br> [1] 【代表论文】</p><h5 id="➢-经典论文-推荐加-👍-2" tabindex="-1"><a class="header-anchor" href="#➢-经典论文-推荐加-👍-2"><span>➢ 经典论文：（推荐加“👍”）</span></a></h5><p>[1] 【发表于】【年份】【论文名】【团队名】【论文链接】【代码链接】【简介】<br> [2] 👍【发表于】【年份】【论文名】【团队名】【论文链接】【代码链接】【简介】</p><h5 id="➢-经典项目-2" tabindex="-1"><a class="header-anchor" href="#➢-经典项目-2"><span>➢ 经典项目</span></a></h5><p>○ 【名称】【链接】【简介】</p><h3 id="✧-小样本目标检测-few-shot-object-detection-陈泰岳" tabindex="-1"><a class="header-anchor" href="#✧-小样本目标检测-few-shot-object-detection-陈泰岳"><span>✧ 小样本目标检测（Few-shot Object Detection）-&gt; <code>陈泰岳</code></span></a></h3>',10)),a("模版"),e[16]||(e[16]=r('<h4 id="论文汇总-论文汇总链接-例如github上的-awesome-系列-3" tabindex="-1"><a class="header-anchor" href="#论文汇总-论文汇总链接-例如github上的-awesome-系列-3"><span>论文汇总：论文汇总链接，例如GitHub上的 <code>Awesome</code> 系列</span></a></h4><h5 id="➢-推荐综述-4" tabindex="-1"><a class="header-anchor" href="#➢-推荐综述-4"><span>➢ 推荐综述</span></a></h5><p>[1] 【发表于】【年份】【论文名】【团队名】【论文链接】【代码链接】【简介】<br> [2] 【知乎/微信/博客等网页链接】【内容简介】</p><h5 id="➢-优秀团队-学术大佬-5" tabindex="-1"><a class="header-anchor" href="#➢-优秀团队-学术大佬-5"><span>➢ 优秀团队 / 学术大佬</span></a></h5><p>■ 【团队名】【团队链接】【所属机构】【细化方向】<br> [1] 【代表论文】<br> ■ 【大佬名】【个人主页】【所属机构】【细化方向】<br> [1] 【代表论文】</p><h5 id="➢-经典论文-推荐加-👍-3" tabindex="-1"><a class="header-anchor" href="#➢-经典论文-推荐加-👍-3"><span>➢ 经典论文：（推荐加“👍”）</span></a></h5><p>[1] 【发表于】【年份】【论文名】【团队名】【论文链接】【代码链接】【简介】<br> [2] 👍【发表于】【年份】【论文名】【团队名】【论文链接】【代码链接】【简介】</p><h5 id="➢-经典项目-3" tabindex="-1"><a class="header-anchor" href="#➢-经典项目-3"><span>➢ 经典项目</span></a></h5><p>○ 【名称】【链接】【简介】</p><blockquote><p>参考文档：</p><ol><li>【在iiplab做科研】 <a href="https://docs.qq.com/pdf/DR3NNa2xqU0Rld1B2" target="_blank" rel="noopener noreferrer">https://docs.qq.com/pdf/DR3NNa2xqU0Rld1B2</a>，特别感谢～</li></ol></blockquote>',10))])}const f=l(c,[["render",g]]),w=JSON.parse(`{"path":"/browser/learning/README%20(2).html","title":"ReadMe🧐","lang":"en-US","frontmatter":{"description":"ReadMe🧐 介绍 该文档项目为 Happy Learning 小组的科研入门资料。 ReadMe仅包含基础内容，可作为索引使用，详细内容请见对应文档。 项目地址: http://10.16.104.13:1805/happy-learning/tohappylearning 小组老大 冷佳旭 https://faculty.cqupt.edu.c...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"ReadMe🧐\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-17T15:06:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Hongyi Wang\\",\\"url\\":\\"https://wanghy1997.github.io/wanghy1997\\"}]}"],["meta",{"property":"og:url","content":"https://wanghy1997.github.io/wanghy1997/browser/learning/README%20(2).html"}],["meta",{"property":"og:site_name","content":"Hongyi's Blog"}],["meta",{"property":"og:title","content":"ReadMe🧐"}],["meta",{"property":"og:description","content":"ReadMe🧐 介绍 该文档项目为 Happy Learning 小组的科研入门资料。 ReadMe仅包含基础内容，可作为索引使用，详细内容请见对应文档。 项目地址: http://10.16.104.13:1805/happy-learning/tohappylearning 小组老大 冷佳旭 https://faculty.cqupt.edu.c..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-09-17T15:06:21.000Z"}],["meta",{"property":"article:modified_time","content":"2025-09-17T15:06:21.000Z"}]]},"git":{"createdTime":1758121581000,"updatedTime":1758121581000,"contributors":[{"name":"whymbp","username":"whymbp","email":"why_6267@163.com","commits":1,"url":"https://github.com/whymbp"}]},"readingTime":{"minutes":14.53,"words":4360},"filePathRelative":"browser/learning/README (2).md","excerpt":"\\n<h2>介绍</h2>\\n<p>该文档项目为 Happy Learning 小组的科研入门资料。<br>\\nReadMe仅包含基础内容，可作为索引使用，详细内容请见对应文档。<br>\\n项目地址: <a href=\\"http://10.16.104.13:1805/happy-learning/tohappylearning\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">http://10.16.104.13:1805/happy-learning/tohappylearning</a></p>\\n<h2>小组老大</h2>\\n<p>冷佳旭  <a href=\\"https://faculty.cqupt.edu.cn/lengjiaxu/zh_CN/index.htm\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://faculty.cqupt.edu.cn/lengjiaxu/zh_CN/index.htm</a></p>","autoDesc":true}`);export{f as comp,w as data};
