import{_ as l}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as d,b as r,a as n,e as a,d as o,f as i,w as s,r as p,o as h}from"./app-BSlDOcH0.js";const c={};function g(u,e){const t=p("RouteLink");return h(),d("div",null,[e[10]||(e[10]=r('<h1 id="readmeğŸ§" tabindex="-1"><a class="header-anchor" href="#readmeğŸ§"><span>ReadMeğŸ§</span></a></h1><h2 id="ä»‹ç»" tabindex="-1"><a class="header-anchor" href="#ä»‹ç»"><span>ä»‹ç»</span></a></h2><p>è¯¥æ–‡æ¡£é¡¹ç›®ä¸º Happy Learning å°ç»„çš„ç§‘ç ”å…¥é—¨èµ„æ–™ã€‚<br> ReadMeä»…åŒ…å«åŸºç¡€å†…å®¹ï¼Œå¯ä½œä¸ºç´¢å¼•ä½¿ç”¨ï¼Œè¯¦ç»†å†…å®¹è¯·è§å¯¹åº”æ–‡æ¡£ã€‚<br> é¡¹ç›®åœ°å€: <a href="http://10.16.104.13:1805/happy-learning/tohappylearning" target="_blank" rel="noopener noreferrer">http://10.16.104.13:1805/happy-learning/tohappylearning</a></p><h2 id="å°ç»„è€å¤§" tabindex="-1"><a class="header-anchor" href="#å°ç»„è€å¤§"><span>å°ç»„è€å¤§</span></a></h2><p>å†·ä½³æ—­ <a href="https://faculty.cqupt.edu.cn/lengjiaxu/zh_CN/index.htm" target="_blank" rel="noopener noreferrer">https://faculty.cqupt.edu.cn/lengjiaxu/zh_CN/index.htm</a></p><h2 id="ä½¿ç”¨æŒ‡å—" tabindex="-1"><a class="header-anchor" href="#ä½¿ç”¨æŒ‡å—"><span>ä½¿ç”¨æŒ‡å—</span></a></h2><p>å¹³å°ä½¿ç”¨æŒ‡å—è¯·è§ <a href="http://10.16.104.13:1805/root/users-guide" target="_blank" rel="noopener noreferrer">http://10.16.104.13:1805/root/users-guide</a></p><p>æ­¤ <code>README</code> æ–‡æ¡£ä»…ä½œä¸ºç´¢å¼•, ä¸»è¦å†…å®¹ä¸ºå…¥é—¨åŸºç¡€çŸ¥è¯†å’Œå„ç ”ç©¶æ–¹å‘ç®€ä»‹ï¼Œè¯¦ç»†ç¬”è®°è¯·è§ <code>1.ç§‘ç ”æ–¹å‘</code> æ–‡ä»¶å¤¹ã€‚</p><hr><h2 id="æ–‡æ¡£æ€»è§ˆ" tabindex="-1"><a class="header-anchor" href="#æ–‡æ¡£æ€»è§ˆ"><span>æ–‡æ¡£æ€»è§ˆ</span></a></h2><p>[toc]</p><hr><h2 id="æ–°ç”Ÿå…¥é—¨" tabindex="-1"><a class="header-anchor" href="#æ–°ç”Ÿå…¥é—¨"><span>æ–°ç”Ÿå…¥é—¨</span></a></h2>',13)),n("ol",null,[e[2]||(e[2]=n("li",null,"é¦–å…ˆå­¦ä¹  Andrew Ng çš„ Machine Learning åœ¨çº¿è¯¾ç¨‹ï¼Œè¦åšè¯¾åä½œä¸š(MATLAB)ï¼›",-1)),e[3]||(e[3]=n("li",null,"å…¶æ¬¡å­¦ä¹  Andrew Ng çš„ Deep Learning åœ¨çº¿è¯¾ç¨‹ï¼Œè¦åšè¯¾åä½œä¸šï¼ˆPython+tensorflowï¼‰ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯èƒ½éœ€è¦èŠ±3-5å¤©å­¦ä¹ pythonçš„åŸºç¡€è¯­æ³•ï¼›",-1)),e[4]||(e[4]=n("li",null,"å­¦ä¹ CS231nåœ¨çº¿è¯¾ç¨‹ï¼Œå¹¶å®Œæˆå¤§ä½œä¸šã€‚",-1)),n("li",null,[e[1]||(e[1]=o("æ·±åº¦å­¦ä¹ åŸºç¡€ç»ƒä¹ åŠç»å…¸æ¨¡å‹ ",-1)),i(t,{to:"/browser/learning/3.%E5%85%A5%E9%97%A8%E5%AE%9E%E8%B7%B5/Torch_learning/"},{default:s(()=>[...e[0]||(e[0]=[o("é“¾æ¥å…¥å£ğŸ”—, ç‚¹å‡»è¿›å…¥ğŸ”—",-1)])]),_:1})]),e[5]||(e[5]=n("li",null,[n("a",{href:"https://mofanpy.com/",target:"_blank",rel:"noopener noreferrer"},"https://mofanpy.com/")],-1))]),e[11]||(e[11]=r('<h2 id="ç§‘ç ”åŸºç¡€" tabindex="-1"><a class="header-anchor" href="#ç§‘ç ”åŸºç¡€"><span>ç§‘ç ”åŸºç¡€</span></a></h2><ol><li>åŸºç¡€è¯¾ç¨‹ï¼šæœ¬ç¡•æœŸé—´é‡ç‚¹æŒæ¡ <code>ã€Šé«˜ç­‰æ•°å­¦ã€‹</code> <code>ã€Šçº¿æ€§ä»£æ•°ã€‹</code> <code>ã€Šæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ã€‹</code> <code>ã€ŠçŸ©é˜µè®ºã€‹</code> <code>ã€Šå‡¸ä¼˜åŒ–ã€‹</code> è¯¾ç¨‹ï¼Œè¿™é‡Œçš„æŒæ¡æ˜¯è¦ç†è§£å®ƒä»¬çš„æœ¬è´¨ã€ä½œç”¨åŠç‰©ç†å‡ ä½•æ„ä¹‰ï¼Œè€Œéåº”ä»˜è€ƒè¯•ã€‚åœ¨å­¦ä¹ æ—¶æŠ›å¼ƒå›½å†…å‡ºç‰ˆçš„æ­»æ¿æ•™æï¼Œæœç´¢å›½å†…å¤–ä¼˜ç§€ä¹¦ç±ã€å…¬å¼€è¯¾ã€è§†é¢‘è¿›è¡Œå­¦ä¹ ã€‚<br> ä¾‹å¦‚ï¼Œã€Šçº¿æ€§ä»£æ•°ã€‹æ¨è<a href="https://www.bilibili.com/video/BV1ys411472E" target="_blank" rel="noopener noreferrer">ã€Šçº¿æ€§ä»£æ•°çš„æœ¬è´¨ã€‹ç³»åˆ—è§†é¢‘</a><a href="https://www.bilibili.com/video/BV1ys411472E" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1ys411472E</a>ï¼Œä»¥åŠGilbert Strang çš„éº»çœç†å·¥å¤§å­¦å…¬å¼€è¯¾è§†é¢‘<a href="https://www.bilibili.com/video/BV1zx411g7gq" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1zx411g7gq</a>ã€‚</li><li>æœºå™¨å­¦ä¹ ï¼šæ¨èå—äº¬å¤§å­¦å‘¨å¿—åè€å¸ˆçš„è¥¿ç“œä¹¦ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼Œåä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤ä¸»ä»»æèˆªè€å¸ˆçš„ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ï¼Œæ–¯å¦ç¦å¤§å­¦Andrew NGï¼ˆå´æ©è¾¾ï¼‰è€å¸ˆçš„æœºå™¨å­¦ä¹ è¯¾ç¨‹è§†é¢‘<a href="https://www.bilibili.com/video/BV164411b7dx" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV164411b7dx</a>ã€‚</li><li>æ·±åº¦å­¦ä¹ ï¼šæ¨èGoogleç ”ç©¶ç§‘å­¦å®¶Ian Goodfellowï¼ˆGenerative Adversarial Netsæå‡ºè€…ï¼‰çš„èŠ±ä¹¦ã€ŠDeep Learningã€‹ï¼ˆæœ‰ä¸­æ–‡è¯‘æœ¬äººæ°‘é‚®ç”µå‡ºç‰ˆç¤¾ï¼‰ï¼Œæ–¯å¦ç¦å¤§å­¦æé£é£ï¼ˆImageNetçš„æå‡ºè€…ï¼‰è€å¸ˆçš„cs231nè®¡ç®—æœºè§†è§‰è¯¾ç¨‹<a href="https://www.bilibili.com/video/BV1nJ411z7fe" target="_blank" rel="noopener noreferrer">https://www.bilibili.com/video/BV1nJ411z7fe</a>ã€‚</li><li>ç§‘ç ”ç¯å¢ƒï¼šæ“ä½œç³»ç»ŸLinuxï¼Œä¸»è¦ç¼–ç¨‹è¯­è¨€Pythonï¼Œä¸»è¦æ·±åº¦å­¦ä¹ æ¡†æ¶Pytorchã€‚å› æ­¤ï¼Œè¦æŒæ¡Linuxçš„ä½¿ç”¨æ–¹æ³•ä¸å¸¸ç”¨å‘½ä»¤ï¼Œç†Ÿç»ƒä½¿ç”¨Pythonè¯­è¨€ç¼–ç¨‹ï¼Œç†Ÿç»ƒä½¿ç”¨Pytorchæ·±åº¦å­¦ä¹ æ¡†æ¶æ­å»ºç½‘ç»œä¸æ·±åº¦å­¦ä¹ ç®—æ³•ã€‚<br> a. Linux å­¦ä¹ ï¼š<a href="https://www.runoob.com/linux/linux-tutorial.html" target="_blank" rel="noopener noreferrer">https://www.runoob.com/linux/linux-tutorial.html</a>ï¼›<br> b. Python å­¦ä¹ ï¼š<a href="https://www.runoob.com/python3/python3-tutorial.html" target="_blank" rel="noopener noreferrer">https://www.runoob.com/python3/python3-tutorial.html</a>ï¼›<br> c. Python ç¬¬ä¸‰æ–¹åº“ Pillowï¼ˆå¤„ç†å›¾åƒï¼‰å­¦ä¹ ï¼š<a href="https://pillow-cn.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener noreferrer">https://pillow-cn.readthedocs.io/zh_CN/latest/</a>ï¼›<br> d. Python ç¬¬ä¸‰æ–¹åº“ Numpyï¼ˆCPUå¤„ç†å¤šç»´å¼ é‡ï¼‰å­¦ä¹ ï¼š<a href="https://www.runoob.com/numpy/numpy-tutorial.html" target="_blank" rel="noopener noreferrer">https://www.runoob.com/numpy/numpy-tutorial.html</a>ï¼›<br> e. æ·±åº¦å­¦ä¹ æ¡†æ¶Pytorchï¼ˆä½¿ç”¨Pythonç¼–ç¨‹ï¼ŒGPUå¤„ç†å¤šç»´å¼ é‡ï¼Œæä¾›æ·±åº¦å­¦ä¹ APIï¼‰å­¦ä¹ ï¼š<a href="https://pytorch.apachecn.org/docs/1.4/" target="_blank" rel="noopener noreferrer">https://pytorch.apachecn.org/docs/1.4/</a>ã€‚</li></ol><h2 id="ç§‘ç ”å·¥å…·" tabindex="-1"><a class="header-anchor" href="#ç§‘ç ”å·¥å…·"><span>ç§‘ç ”å·¥å…·</span></a></h2>',3)),n("ol",null,[e[9]||(e[9]=r('<li>ç§‘å­¦ä¸Šç½‘ï¼šä¸€ä¸ªå¯ç”¨ç¨³å®šçš„ä¸Šè°·æ­Œï¼ˆç¿»å¢™ï¼‰å·¥å…·æ˜¯å¿…é¡»çš„ï¼Œæ¨èï¼š<a href="https://geckoiplc.org/register?aff=nxkYWh33" target="_blank" rel="noopener noreferrer">https://geckoiplc.org/register?aff=nxkYWh33</a>ï¼Œä½¿ç”¨æ–¹æ³•è¯¥ç½‘ç«™æœ‰æ•™ç¨‹</li><li>æ£€ç´¢è®ºæ–‡ï¼šGoogleScholarï¼š<a href="https://scholar.google.com/" target="_blank" rel="noopener noreferrer">https://scholar.google.com/</a>ï¼ˆéœ€ç¿»å¢™ï¼‰</li><li>ä¸‹è½½è®ºæ–‡ï¼š<br> a. IEEEï¼š<a href="https://ieeexplore.ieee.org/Xplore/home.jsp" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/Xplore/home.jsp</a>ï¼›<br> b. ScienceDirectï¼š<a href="https://www.sciencedirect.com/" target="_blank" rel="noopener noreferrer">https://www.sciencedirect.com/</a>ï¼›<br> c. SpringerLinkï¼š<a href="https://link.springer.com/" target="_blank" rel="noopener noreferrer">https://link.springer.com/</a>ï¼›<br> d. arXivï¼š<a href="https://arxiv.org/" target="_blank" rel="noopener noreferrer">https://arxiv.org/</a>å å‘ç½‘ç«™ï¼Œæœ€æ–°çš„æ–‡ç« ä¼šå‘è¡¨åœ¨ä¸Šé¢å å‘ï¼Œä½†æ˜¯è‰¯è ä¸é½ï¼Œå…è´¹ä¸‹è½½ï¼Œç¿»å¢™ä¼šå¿«ï¼›<br> e. Sci-Hubï¼š<a href="https://www.sci-hub.ren/" target="_blank" rel="noopener noreferrer">https://www.sci-hub.ren/</a>ä¸åœ¨å­¦æ ¡å†…åˆæƒ³ä¸‹è½½IEEEç­‰ç‰ˆæƒæ”¶è´¹æ–‡ç« ï¼ŸSci-Hubæ˜¯ç¥å™¨ï¼Œç½‘ç«™å®£è¨€...to remove all the barriers in the way of scienceï¼Œåœ°å€å¯èƒ½ä¼šå˜ï¼Œå› ä¸ºä¸€ç›´è¢«å‘Šã€‚</li><li>ç®¡ç†è®ºæ–‡ï¼šMendelyï¼š<a href="https://www.mendeley.com/" target="_blank" rel="noopener noreferrer">https://www.mendeley.com/</a>ï¼›Endnotesï¼š<a href="https://www.endnote.com/" target="_blank" rel="noopener noreferrer">https://www.endnote.com/</a>ï¼›Zoteroï¼š<a href="https://www.zotero.org/" target="_blank" rel="noopener noreferrer">https://www.zotero.org/</a>ï¼›è‡ªå·±é€‰æ‹©ã€‚</li><li>æ’°å†™è®ºæ–‡ï¼šLaTexç¯å¢ƒå®‰è£…ï¼š<a href="http://tug.org/texlive/" target="_blank" rel="noopener noreferrer">http://tug.org/texlive/</a>ï¼›æŒ‰ç€ç½‘ç«™æŒ‡å¯¼æ¥ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨åœ¨çº¿çš„LaTexç¼–è¾‘å·¥å…·å¦‚Overleafï¼š<a href="https://www.overleaf.com/login" target="_blank" rel="noopener noreferrer">https://www.overleaf.com/login</a>ã€‚</li><li>è®ºæ–‡ä½œå›¾ï¼šPPTã€Visio(Windows)ã€OmniGraffle(Mac)ï¼›<br> ç›´æ¥ä¿å­˜æˆPDFå°±æ˜¯çŸ¢é‡å›¾ï¼Œä½¿ç”¨Adobe Acrobatå°†å›¾ç‰‡å¤šä½™éƒ¨åˆ†è£å‰ªã€‚</li><li>ä»£ç ç¼–è¾‘ï¼šVSCodeï¼ˆVisual Studio Codeï¼‰ï¼š<a href="https://code.visualstudio.com/" target="_blank" rel="noopener noreferrer">https://code.visualstudio.com/</a>ï¼›<br> ä½ æƒ³è¦çš„ä»£ç ç¼–è¾‘åŠŸèƒ½éƒ½èƒ½é€šè¿‡VSCode+æ’ä»¶å®ç°ã€‚<br> å¸¸ç”¨VSCodeæ’ä»¶ï¼šLaTex Workshopï¼ˆç”¨VSCodeå†™è®ºæ–‡ï¼Œå‰æå¾—å…ˆè£…å¥½LaTexç¯å¢ƒï¼‰ã€Remote-SSHï¼ˆç”¨VSCodeç›´æ¥æµè§ˆç¼–è¾‘è¿œç¨‹æœåŠ¡å™¨ä¸Šçš„ä»£ç å’Œå›¾ç‰‡ï¼‰ã€One Dark Proï¼ˆè®©VSCodeæ›´å¥½çœ‹ï¼‰ã€Bracket Pair Colorizer 2ï¼ˆè®©ä»£ç æ¯ä¸€çº§æ‹¬å·å¸¦ä¸åŒçš„é¢œè‰²ï¼Œæ‹¬å·å†ä¹Ÿä¸ä¼šä¸æˆå¯¹æŠ¥é”™äº†ï¼‰ã€Code Spell Checkerï¼ˆæ£€æŸ¥ä½ çš„æ‹¼å†™æ˜¯å¦æœ‰é”™è¯¯ï¼‰ã€indent-rainbowï¼ˆä¸åŒçº§çš„ç¼©è¿›å¸¦ä¸åŒé¢œè‰²ï¼Œå¯¹Pythonæ¥è¯´å¾ˆæœ‰ç”¨ï¼‰ã€Pythonï¼ˆå¯ä»¥ç›´æ¥åœ¨VSCodeè°ƒè¯•Pythonç¨‹åºï¼Œå‰æå¾—å…ˆè£…å¥½Pythonç¯å¢ƒï¼‰ã€‚</li>',7)),n("li",null,[e[7]||(e[7]=o("æœåŠ¡å™¨åå°è¿è¡Œç®¡ç†å·¥å…·tmux ",-1)),i(t,{to:"/browser/learning/2.%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7tmux.html"},{default:s(()=>[...e[6]||(e[6]=[o("é“¾æ¥å…¥å£ğŸ”—, ç‚¹å‡»è¿›å…¥ğŸ”—",-1)])]),_:1}),e[8]||(e[8]=o("ã€‚",-1))])]),e[12]||(e[12]=r('<h2 id="ç§‘ç ”æ–¹å‘" tabindex="-1"><a class="header-anchor" href="#ç§‘ç ”æ–¹å‘"><span>ç§‘ç ”æ–¹å‘</span></a></h2><h3 id="âœ§-è¡Œäººæ£€æµ‹-person-detection-é«˜å³°-åšå£«" tabindex="-1"><a class="header-anchor" href="#âœ§-è¡Œäººæ£€æµ‹-person-detection-é«˜å³°-åšå£«"><span>âœ§ è¡Œäººæ£€æµ‹ï¼ˆPerson Detectionï¼‰-&gt; <code>é«˜å³° (åšå£«)</code></span></a></h3><h4 id="è®ºæ–‡æ±‡æ€»-è®ºæ–‡æ±‡æ€»é“¾æ¥-ä¾‹å¦‚githubä¸Šçš„-awesome-ç³»åˆ—" tabindex="-1"><a class="header-anchor" href="#è®ºæ–‡æ±‡æ€»-è®ºæ–‡æ±‡æ€»é“¾æ¥-ä¾‹å¦‚githubä¸Šçš„-awesome-ç³»åˆ—"><span>è®ºæ–‡æ±‡æ€»ï¼šè®ºæ–‡æ±‡æ€»é“¾æ¥ï¼Œä¾‹å¦‚GitHubä¸Šçš„ <code>Awesome</code> ç³»åˆ—</span></a></h4><h5 id="â¢-æ¨èç»¼è¿°" tabindex="-1"><a class="header-anchor" href="#â¢-æ¨èç»¼è¿°"><span>â¢ æ¨èç»¼è¿°</span></a></h5><p>[1] From Handcrafted to Deep Features for Pedestrian Detection: A Survey (TPAMI2021) æ¶µç›–äº†2020å¹´åŠä»¥å‰ä»ä¼ ç»Ÿåˆ°æ·±åº¦å­¦ä¹ çš„è¡Œäººæ£€æµ‹æ‰€æœ‰çŸ¥è¯†ã€‚ä½†æ˜¯ï¼Œ2020Detræ¨ªç©ºå‡ºä¸–,2021çªé£çŒ›è¿›ï¼Œ2022åŸºäºDetræ€æƒ³çš„ç¬¬ä¸€ç¯‡è¡Œäººæ£€æµ‹é¡¶ä¼šé—®ä¸–,ä¸ªäººè§‰å¾—è¿™æ˜¯é‡Œç¨‹ç¢‘å¼çš„ä½œå“ã€‚å› ä¸ºè‡ªFaster-RCNNé—®ä¸–ä»¥æ¥ï¼Œè¡Œäººæ£€æµ‹é¢†åŸŸç™¾åˆ†ä¹‹80ä»¥ä¸Šçš„æ–‡ç« éƒ½æ˜¯Faster-RCNNçš„å˜ç§ã€‚æœªæ¥å¯èƒ½Detræ›¿ä»£Faster-RCNNï¼Œæˆä¸ºè¡Œäººæ£€æµ‹é¢†åŸŸçš„Baselineã€‚ç›®å‰DETRç›¸å…³èµ„æ–™éå¸¸å°‘ï¼Œè¿™éƒ¨åˆ†çŸ¥è¯†éœ€è¦è‡ªå·±æƒ³åŠæ³•ã€‚</p><h5 id="â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬" tabindex="-1"><a class="header-anchor" href="#â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬"><span>â¢ ä¼˜ç§€å›¢é˜Ÿ / å­¦æœ¯å¤§ä½¬</span></a></h5><p>â–  ã€æ—·è§†å›¢é˜Ÿã€‘<br> [1] Detection in Crowded Scenes: One Proposal, Multiple Predictions. CVPR2020<br> [2] End-to-End Object Detection with Fully Convolutional Network. CVPR2021<br> [3] Progressive End-to-End Object Detection in Crowded Scenes. CVPR2022<br> â–  ã€Peize Sunã€‘ã€é¦™æ¸¯å¤§å­¦åœ¨è¯»åšå£«ã€‘<br> [1] Sparse r-cnn: End-to-end object detection with learnable proposals. CVPR2021<br> [2] What Makes for End-to-End Object Detection? ICML2021</p><h5 id="â¢-ç»å…¸é¡¹ç›®" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸é¡¹ç›®"><span>â¢ ç»å…¸é¡¹ç›®</span></a></h5><p>â—‹ ä¸Šè¿°äº”ç¯‡è®ºæ–‡éƒ½å¼€æºäº†ä»£ç ï¼Œåœ°å€åœ¨è®ºæ–‡ä¸­ã€‚ç¬¬ä¸€ç¯‡è®ºæ–‡å¼€æºçš„ä»£ç ä¸­åŒ…å«äº†çº¯pytorchå®ç°çš„Faster-RCNNå’ŒRetinanetéå¸¸é€‚åˆåˆå­¦è€…å…¥é—¨åŸºäºCNNçš„ç›®æ ‡æ£€æµ‹ç®—æ³•ã€‚<br> â—‹ æˆ‘ä¸Šä¼ äº†ä¸€ä¸ªç®€æ˜“ç‰ˆçš„DETRçº¯pytorchå®ç°åœ¨æˆ‘çš„ç§‘ç ”æ–¹å‘æ–‡ä»¶å¤¹ä¸­ï¼Œå¯ä»¥å¾ˆå¥½çš„å­¦ä¹ DETRåŸç†ã€‚<br> â—‹ æ—·è§†å›¢é˜Ÿçš„ç¬¬ä¸‰ç¯‡è®ºæ–‡æ˜¯Peize Sunç¬¬ä¸€ç¯‡è®ºæ–‡çš„æ”¹è¿›ï¼Œé€šè¿‡é˜…è¯»ä»£ç å¯ä»¥ä»ä¸€ä¸ªè§’åº¦äº†è§£ç›®å‰ç ”ç©¶è€…æ”¹è¿›Detrç®—æ³•çš„æ€è·¯ã€‚</p><h3 id="âœ§-è¡Œäººé‡è¯†åˆ«-person-re-identification-æ±ªæµ·æ¶›" tabindex="-1"><a class="header-anchor" href="#âœ§-è¡Œäººé‡è¯†åˆ«-person-re-identification-æ±ªæµ·æ¶›"><span>âœ§ è¡Œäººé‡è¯†åˆ«ï¼ˆPerson Re-identificationï¼‰-&gt; <code>æ±ªæµ·æ¶›</code></span></a></h3><h4 id="è®ºæ–‡æ±‡æ€»-https-github-com-bismex-awesome-person-re-identification-è¯¥-repo-å†…æœ‰ç›®å‰-video-reid-æ–¹å‘çš„é¡¶ä¼šæ‰€æœ‰è®ºæ–‡æ±‡æ€»-åŒ…æ‹¬åŸºæœ¬åˆ†ç±»ã€-å¸¸ç”¨æ•°æ®åº“ä¸‹è½½ã€å¸¸ç”¨-code" tabindex="-1"><a class="header-anchor" href="#è®ºæ–‡æ±‡æ€»-https-github-com-bismex-awesome-person-re-identification-è¯¥-repo-å†…æœ‰ç›®å‰-video-reid-æ–¹å‘çš„é¡¶ä¼šæ‰€æœ‰è®ºæ–‡æ±‡æ€»-åŒ…æ‹¬åŸºæœ¬åˆ†ç±»ã€-å¸¸ç”¨æ•°æ®åº“ä¸‹è½½ã€å¸¸ç”¨-code"><span>è®ºæ–‡æ±‡æ€»: <a href="https://github.com/bismex/Awesome-person-re-identification" target="_blank" rel="noopener noreferrer">https://github.com/bismex/Awesome-person-re-identification</a>ï¼Œè¯¥ repo å†…æœ‰ç›®å‰ video reid æ–¹å‘çš„é¡¶ä¼šæ‰€æœ‰è®ºæ–‡æ±‡æ€»ï¼ŒåŒ…æ‹¬åŸºæœ¬åˆ†ç±»ã€ å¸¸ç”¨æ•°æ®åº“ä¸‹è½½ã€å¸¸ç”¨ code</span></a></h4><h5 id="â¢-æ¨èç»¼è¿°-1" tabindex="-1"><a class="header-anchor" href="#â¢-æ¨èç»¼è¿°-1"><span>â¢ æ¨èç»¼è¿°</span></a></h5><p>[1] Deep Learning for Person Re-identification: A Survey and Outlook, TPAMI 2020(è¿‘æœŸçš„ reid å·¥ä½œæ±‡æ€»)<br> [2] Person Re-identification: Past, Present and Future, arXiv 2016(æ—©æœŸçš„ reid å·¥ä½œæ±‡æ€»)</p><ul><li>åŸºæœ¬çœ‹ä¸Šä¸Šé¢ä¸¤ä¸ª survey å¯¹ç›®å‰ reid çš„å‘å±•è„‰ç»œå°±å¯ä»¥æœ‰ä¸€ä¸ªæ¸…æ™°çš„æŠŠæ¡äº†</li><li>çœ‹è®ºæ–‡çš„æ—¶å€™ related work é‡Œé¢çš„è®ºæ–‡éƒ½å¯ä»¥å‚è€ƒçœ‹ä¸€çœ‹</li><li>ä¸€äº›å¿…å¤‡çš„æŠ€æœ¯ç‚¹:bagtricks, agw, sbs, pcb, mgn, arcface</li></ul><h5 id="â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-1" tabindex="-1"><a class="header-anchor" href="#â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-1"><span>â¢ ä¼˜ç§€å›¢é˜Ÿ / å­¦æœ¯å¤§ä½¬</span></a></h5><p>â–  éƒ‘ä¼Ÿè¯— Wei-Shi Zheng ä¸­å±±å¤§å­¦æ™ºèƒ½ç§‘å­¦ä¸ç³»ç»Ÿå®éªŒå®¤(iSEE) <a href="https://www.isee-ai.cn/~zhwshi/" target="_blank" rel="noopener noreferrer">https://www.isee-ai.cn/~zhwshi/</a><br> [1] J. Yang, W. -S. Zheng, Q. Yang, Y. -C. Chen and Q. Tian, &quot;Spatial-Temporal Graph Convolutional Network for Video-Based Person Re-Identification,&quot; 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 3286-3296, doi: 10.1109/CVPR42600.2020.00335.<br> [2] J. Yang et al., &quot;Learning to Know Where to See: A Visibility-Aware Approach for Occluded Person Re-identification,&quot; 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 11865-11874, doi: 10.1109/ICCV48922.2021.01167.</p><p>â–  å¸¸è™¹ Chang Hong ä¸­å›½ç§‘å­¦é™¢è®¡ç®—æœºæŠ€æœ¯ç ”ç©¶é™¢(CAS) <a href="https://people.ucas.ac.cn/~changhong" target="_blank" rel="noopener noreferrer">https://people.ucas.ac.cn/~changhong</a><br> [1] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan and X. Chen, &quot;VRSTC: Occlusion-Free Video Person Re-Identification,&quot; 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 7176-7185, doi: 10.1109/CVPR.2019.00735.<br> [2] R. Hou, H. Chang, B. Ma, R. Huang and S. Shan, &quot;BiCnet-TKS: Learning Efficient Spatial-Temporal Representation for Video Person Re-Identification,&quot; 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 2014-2023, doi: 10.1109/CVPR46437.2021.00205.</p><h5 id="â¢-å¯è§å…‰-reid-ä»£è¡¨è®ºæ–‡" tabindex="-1"><a class="header-anchor" href="#â¢-å¯è§å…‰-reid-ä»£è¡¨è®ºæ–‡"><span>â¢ å¯è§å…‰ reid ä»£è¡¨è®ºæ–‡</span></a></h5><p>[1] Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification, ECCV2020<br> [2] Identity-Guided Human Semantic Parsing for Person Re-Identification, ECCV2020<br> [3] Faster Person Re-Identification, ECCV20 (è½»é‡çº§ reidï¼Œ <a href="https://github.com/wangguanan/light-reid" target="_blank" rel="noopener noreferrer">https://github.com/wangguanan/light-reid</a> )<br> [4] Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive Person Re-Identification, ECCV2020<br> [5] Re-Ranking Person Re-Identification With k-Reciprocal Encodingï¼Œ CVPR2017(å°† reranking åº”ç”¨åˆ° reidï¼Œæ¶¨ç‚¹æ˜¾è‘—)<br> [6] Person Re-Identification in the Wild, CVPR2017(ä»æ£€æµ‹åˆ°è¯†åˆ«)</p><h5 id="â¢-å¼‚è´¨-reid-ä»£è¡¨è®ºæ–‡" tabindex="-1"><a class="header-anchor" href="#â¢-å¼‚è´¨-reid-ä»£è¡¨è®ºæ–‡"><span>â¢ å¼‚è´¨ reid ä»£è¡¨è®ºæ–‡</span></a></h5><p>[1] Hi-CMD: Hierarchical Cross-Modality Disentanglement for Visible-Infrared Person Re-Identificationï¼ŒCVPR2020<br> [2] Cross-Modality Person Re-Identification With Shared-Specific Feature Transfer, CVPR2020<br> [3] RGB-Infrared Cross-Modality Person Re-Identification via Joint Pixel and Feature Alignment, ICCV2019<br> [4] Infrared-Visible Cross-Modal Person Re-Identification with an X Modality, AAAI2020</p><h5 id="â¢-å¸¸ç”¨å¼€æºé¡¹ç›®" tabindex="-1"><a class="header-anchor" href="#â¢-å¸¸ç”¨å¼€æºé¡¹ç›®"><span>â¢ å¸¸ç”¨å¼€æºé¡¹ç›®</span></a></h5><ul><li>strong baseline: <a href="https://github.com/michuanhaohao/reid-strong-baseline" target="_blank" rel="noopener noreferrer">https://github.com/michuanhaohao/reid-strong-baseline</a></li><li>FAST-REID: <a href="https://github.com/JDAI-CV/fast-reid" target="_blank" rel="noopener noreferrer">https://github.com/JDAI-CV/fast-reid</a></li></ul><h3 id="âœ§-è¶…åˆ†è¾¨ç‡é‡å»º-super-resolution" tabindex="-1"><a class="header-anchor" href="#âœ§-è¶…åˆ†è¾¨ç‡é‡å»º-super-resolution"><span>âœ§ è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSuper-resolutionï¼‰</span></a></h3>',24)),a("æ¨¡ç‰ˆ"),e[13]||(e[13]=r('<h4 id="è®ºæ–‡æ±‡æ€»-è®ºæ–‡æ±‡æ€»é“¾æ¥-ä¾‹å¦‚githubä¸Šçš„-awesome-ç³»åˆ—-1" tabindex="-1"><a class="header-anchor" href="#è®ºæ–‡æ±‡æ€»-è®ºæ–‡æ±‡æ€»é“¾æ¥-ä¾‹å¦‚githubä¸Šçš„-awesome-ç³»åˆ—-1"><span>è®ºæ–‡æ±‡æ€»ï¼šè®ºæ–‡æ±‡æ€»é“¾æ¥ï¼Œä¾‹å¦‚GitHubä¸Šçš„ <code>Awesome</code> ç³»åˆ—</span></a></h4><h5 id="â¢-æ¨èç»¼è¿°-2" tabindex="-1"><a class="header-anchor" href="#â¢-æ¨èç»¼è¿°-2"><span>â¢ æ¨èç»¼è¿°</span></a></h5><p>[1] Video Super Resolution Based on Deep Learning: A comprehensive survey<br> [2] Deep Learning for Image Super-resolution: A Survey<br> [3] Blind Image Super-Resolution: A Survey and Beyond</p><ul><li>ä¸‰ç¯‡ç»¼è¿°åˆ†åˆ«ä»£è¡¨è¶…åˆ†é¢†åŸŸä¸­çš„ä¸‰ä¸ªå°é¢†åŸŸï¼šè§†é¢‘è¶…åˆ†è¾¨ç‡ã€å•å›¾è¶…åˆ†è¾¨ç‡ã€ç›²å›¾åƒè¶…åˆ†è¾¨ç‡ã€‚</li></ul><h5 id="â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-2" tabindex="-1"><a class="header-anchor" href="#â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-2"><span>â¢ ä¼˜ç§€å›¢é˜Ÿ / å­¦æœ¯å¤§ä½¬</span></a></h5><p>â–  Xintao Wang:ç°ä»»è…¾è®¯ARCå®éªŒå®¤ï¼ˆæ·±åœ³ï¼‰ç ”ç©¶å‘˜ã€‚æ¯•ä¸šäºé¦™æ¸¯ä¸­æ–‡å¤§å­¦å¤šåª’ä½“å®éªŒå®¤ã€‚ä¸»è¦ç ”ç©¶å›¾åƒå’Œè§†é¢‘çš„æ¢å¤ã€‚<br> ä¸ªäººä¸»é¡µï¼š<a href="https://xinntao.github.io/" target="_blank" rel="noopener noreferrer">https://xinntao.github.io/</a><br> [1] Wang X, Chan K C K, Yu K, et al. Edvr: Video restoration with enhanced deformable convolutional networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2019: 0-0.<br> [2] Wang X, Yu K, Dong C, et al. Recovering realistic texture in image super-resolution by deep spatial feature transform[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 606-615.<br> [3] Wang X, Yu K, Wu S, et al. Esrgan: Enhanced super-resolution generative adversarial networks[C]//Proceedings of the European conference on computer vision (ECCV) workshops. 2018: 0-0.<br> â–  Kelvin C.K. Chanï¼šæ–°åŠ å¡å—æ´‹ç†å·¥å¤§å­¦ï¼Œè®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢ï¼Œé¦™æ¸¯ä¸­æ–‡å¤§å­¦å­¦å£«å­¦ä½ã€‚<br> ä¸ªäººä¸»é¡µï¼š<a href="https://ckkelvinchan.github.io/" target="_blank" rel="noopener noreferrer">https://ckkelvinchan.github.io/</a><br> [1] Chan K C K, Wang X, Yu K, et al. BasicVSR: The search for essential components in video super-resolution and beyond[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 4947-4956.<br> [2] Chan K C K, Zhou S, Xu X, et al. BasicVSR++: Improving video super-resolution with enhanced propagation and alignment[J]. arXiv preprint arXiv:2104.13371, 2021.<br> [3] Chan K C K, Wang X, Yu K, et al. Understanding deformable alignment in video super-resolution[J]. arXiv preprint arXiv:2009.07265, 2020, 4(3): 4.</p><h5 id="â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘"><span>â¢ ç»å…¸è®ºæ–‡ï¼šï¼ˆæ¨èåŠ â€œğŸ‘â€ï¼‰</span></a></h5><ul><li><p>Video Super-Resolution<br> [1] Ward C M, Harguess J, Crabb B, et al. Image quality assessment for determining efficacy and limitations of Super-Resolution Convolutional Neural Network (SRCNN)[C]//Applications of Digital Image Processing XL. International Society for Optics and Photonics, 2017, 10396: 1039605.<br> ç®€ä»‹ï¼šè¯¥ç¯‡è®ºæ–‡æ˜¯æ·±åº¦å­¦ä¹ åœ¨è¶…åˆ†è¾¨ç‡çš„å¼€å±±ä¹‹ä½œã€‚æ¯ä¸ªå…¥é—¨SRçš„å­¦è€…éƒ½éœ€è¦ä»è¿™ä¸€ç¯‡è®ºæ–‡å¼€å§‹ã€‚<br> [2] Liu D, Wang Z, Fan Y, et al. Robust video super-resolution with learned temporal dynamics[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 2507-2515.<br> [3] Tian Y, Zhang Y, Fu Y, et al. Tdan: Temporally-deformable alignment network for video super-resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3360-3369.<br> [4] ğŸ‘ã€Wang X, Chan K C K, Yu K, et al. Edvr: Video restoration with enhanced deformable convolutional networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2019: 0-0.<br> [5] Haris M, Shakhnarovich G, Ukita N. Recurrent back-projection network for video super-resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 3897-3906.<br> [6] Chan K C K, Wang X, Yu K, et al. BasicVSR: The search for essential components in video super-resolution and beyond[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 4947-4956.<br> [7] Chan K C K, Wang X, Yu K, et al. Understanding deformable alignment in video super-resolution[J]. arXiv preprint arXiv:2009.07265, 2020, 4(3): 4.</p></li><li><p>Blind Image Super-Resolution<br> [1] Zhang K, Zuo W, Zhang L. Learning a single convolutional super-resolution network for multiple degradations[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 3262-3271.<br> [2] Gu J, Lu H, Zuo W, et al. Blind super-resolution with iterative kernel correction[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 1604-1613.<br> [3] Wei Y, Gu S, Li Y, et al. Unsupervised real-world image super resolution via domain-distance aware training[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 13385-13394.<br> [4] Shocher A, Cohen N, Irani M. â€œzero-shotâ€ super-resolution using deep internal learning[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 3118-3126.<br> [5] Hui Z, Li J, Wang X, et al. Learning the Non-differentiable Optimization for Blind Super-Resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 2093-2102.</p></li></ul><h5 id="â¢-ç»å…¸å¼€æºé¡¹ç›®" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸å¼€æºé¡¹ç›®"><span>â¢ ç»å…¸å¼€æºé¡¹ç›®</span></a></h5><p>â—‹ ã€åç§°ã€‘ã€é“¾æ¥ã€‘ã€ç®€ä»‹ã€‘</p><p>[1] BasicVSR: <a href="https://github.com/xinntao/BasicSR" target="_blank" rel="noopener noreferrer">https://github.com/xinntao/BasicSR</a><br> ç®€ä»‹ï¼šè¯¥é¡¹ç›®é‡ŒåŒ…å«äº†å¾ˆå¤šç»å…¸è®ºæ–‡çš„å¤ç°ï¼Œå› æ­¤å¯ç›´æ¥å°†è¿™ä¸ªå¼€æºä»£ç ç”¨ä¼šå°±å¯ã€‚</p><h3 id="âœ§-è§†é¢‘å¼‚å¸¸æ£€æµ‹-video-anomaly-detection-è°­æ˜åœ®" tabindex="-1"><a class="header-anchor" href="#âœ§-è§†é¢‘å¼‚å¸¸æ£€æµ‹-video-anomaly-detection-è°­æ˜åœ®"><span>âœ§ è§†é¢‘å¼‚å¸¸æ£€æµ‹ (Video Anomaly Detection) -&gt; <code>è°­æ˜åœ®</code></span></a></h3>',12)),a("æ¨¡ç‰ˆ"),e[14]||(e[14]=r('<h4 id="è®ºæ–‡æ±‡æ€»" tabindex="-1"><a class="header-anchor" href="#è®ºæ–‡æ±‡æ€»"><span>è®ºæ–‡æ±‡æ€»ï¼š</span></a></h4><p>[1] <a href="https://github.com/fjchange/awesome-video-anomaly-detection" target="_blank" rel="noopener noreferrer">https://github.com/fjchange/awesome-video-anomaly-detection</a> è¯¥ repo å†…æœ‰ç›®å‰ è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰ æ–¹å‘çš„ä¼˜ç§€è®ºæ–‡æ±‡æ€»ï¼ŒåŒ…æ‹¬åŸºæœ¬åˆ†ç±»ã€ å¸¸ç”¨æ•°æ®åº“ä¸‹è½½ã€ å¼€æºcodeã€ ç»¼è¿°<br> [2] <a href="https://github.com/shot1107/anomaly_detection_papers" target="_blank" rel="noopener noreferrer">https://github.com/shot1107/anomaly_detection_papers</a> è¯¥repo å†…æœ‰å¼‚å¸¸æ£€æµ‹æ¯å¹´é¡¶ä¼šçš„è®ºæ–‡ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼Œå¯å‚è€ƒå€Ÿé‰´ã€‚</p><h5 id="â¢-è®¤è¯†å¼‚å¸¸æ£€æµ‹" tabindex="-1"><a class="header-anchor" href="#â¢-è®¤è¯†å¼‚å¸¸æ£€æµ‹"><span>â¢ è®¤è¯†å¼‚å¸¸æ£€æµ‹</span></a></h5><h6 id="_1-ç®€å•ä»‹ç»-ä»å¼‚å¸¸è¡Œä¸ºæ£€æµ‹-è§†é¢‘å¼‚å¸¸è¡Œä¸ºæ£€æµ‹" tabindex="-1"><a class="header-anchor" href="#_1-ç®€å•ä»‹ç»-ä»å¼‚å¸¸è¡Œä¸ºæ£€æµ‹-è§†é¢‘å¼‚å¸¸è¡Œä¸ºæ£€æµ‹"><span>1. ç®€å•ä»‹ç»ï¼ˆä»å¼‚å¸¸è¡Œä¸ºæ£€æµ‹--&gt; è§†é¢‘å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ï¼‰</span></a></h6><p>[1] å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ç®€ä»‹ï¼š <a href="https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw</a><br> [2] åŸºäºæ·±åº¦å­¦ä¹ çš„å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ä»‹ç»ï¼š<a href="https://mp.weixin.qq.com/s/Aghbz4m1eWFCNGgEy8q6Cg" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/Aghbz4m1eWFCNGgEy8q6Cg</a><br> [3] åŸºäºæ·±åº¦å­¦ä¹ çš„å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ç ”ç©¶ç°çŠ¶ï¼š <a href="https://mp.weixin.qq.com/s/MwpELRlC1cuDgqn4staAzA" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/MwpELRlC1cuDgqn4staAzA</a><br> [4] åŸºäºæ·±åº¦å­¦ä¹ çš„è§†é¢‘å¼‚å¸¸è¡Œä¸ºäº‹ä»¶æ£€æµ‹ç®€ä»‹: <a href="https://mp.weixin.qq.com/s/i3Xw2-ivARnF7rBSFtxugw" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/i3Xw2-ivARnF7rBSFtxugw</a><br> [5] åŸºäºè§†é¢‘çš„å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ç®—æ³•ä»‹ç»: <a href="https://mp.weixin.qq.com/s/Dxsc3oCuO0wYkeFubMfSNw" target="_blank" rel="noopener noreferrer">https://mp.weixin.qq.com/s/Dxsc3oCuO0wYkeFubMfSNw</a></p><h6 id="_2-è®ºæ–‡ç»¼è¿°" tabindex="-1"><a class="header-anchor" href="#_2-è®ºæ–‡ç»¼è¿°"><span>2.è®ºæ–‡ç»¼è¿°ï¼š</span></a></h6><p>[1] é‚¬å¼€ä¿Šç­‰. è§†é¢‘å¼‚å¸¸æ£€æµ‹æŠ€æœ¯ç ”ç©¶è¿›å±•[J]. è®¡ç®—æœºç§‘å­¦ä¸æ¢ç´¢, 2022 ï¼ˆä¸­æ–‡ç»¼è¿°ï¼Œä½†æ²¡æœ‰é‚£ä¹ˆå…¨é¢ï¼Œå¯ä»¥æœ‰ä¸€ä¸ªåˆæ­¥äº†è§£ï¼‰<br> [2] Bharathkumar Ramachandra et al. A survey of single-scene video anomaly detection (TPAMI 2020)</p><h5 id="â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-3" tabindex="-1"><a class="header-anchor" href="#â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-3"><span>â¢ ä¼˜ç§€å›¢é˜Ÿ / å­¦æœ¯å¤§ä½¬</span></a></h5><h6 id="â– -é«˜ç››å-ä¸Šæµ·ç§‘æŠ€å¤§å­¦-è§†è§‰ä¸æ•°æ®æ™ºèƒ½ä¸­å¿ƒ" tabindex="-1"><a class="header-anchor" href="#â– -é«˜ç››å-ä¸Šæµ·ç§‘æŠ€å¤§å­¦-è§†è§‰ä¸æ•°æ®æ™ºèƒ½ä¸­å¿ƒ"><span>â–  é«˜ç››å ä¸Šæµ·ç§‘æŠ€å¤§å­¦ï¼ˆè§†è§‰ä¸æ•°æ®æ™ºèƒ½ä¸­å¿ƒï¼‰</span></a></h6><p>[1] A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework <strong>(ICCV 2017)</strong> --&gt;proposed Shanghaitech dataset.<br> [2] Future Frame Prediction for Anomaly Detection â€“ A New Baseline <strong>(CVPR 2018)</strong><br> [3] Future Frame Prediction for Anomaly Detection <strong>(TPAMI 2022)</strong></p><h6 id="â– -radu-ionescu-securifai-university-of-bucharest" tabindex="-1"><a class="header-anchor" href="#â– -radu-ionescu-securifai-university-of-bucharest"><span>â–  Radu Ionescu SecurifAI/University of Bucharest</span></a></h6><p>[1] Detecting abnormal events in video using Narrowed Normality Clusters <strong>(WACV 2019)</strong><br> [2] Object-centric Auto-encoders and Dummy Anomalies for Abnormal Event Detection in Video <strong>(CVPR 2019)</strong><br> [3] Anomaly Detection in Video via Self-Supervised and Multi-Task Learning <strong>(CVPR 2021)</strong><br> [4] A Background-Agnostic Framework with Adversarial Training for Abnormal Event Detection in Video <strong>(TPAMI 2021)</strong><br> [5] UBnormal New Benchmark for Supervised Open-Set Video Anomaly Detection <strong>(CVPR 2022)</strong><br> [6] Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection <strong>(CVPR 2022)</strong></p><h5 id="â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘-1" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘-1"><span>â¢ ç»å…¸è®ºæ–‡ï¼šï¼ˆæ¨èåŠ â€œğŸ‘â€ï¼‰</span></a></h5><h6 id="â– -unsupervised-vad" tabindex="-1"><a class="header-anchor" href="#â– -unsupervised-vad"><span>â–  Unsupervised VAD</span></a></h6><ul><li><p><strong>Conference Papers</strong><br> [1] Learning Temporal Regularity in Video Sequences <strong>(CVPR 2016)</strong><br> [2] A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework --&gt;<strong>Proposed Shanghaitech dataset.</strong><br> [2] ğŸ‘Future Frame Prediction for Anomaly Detection -- A New Baseline <strong>(CVPR 2018)</strong><br> [3] ğŸ‘Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection <strong>(ICCV 2019)</strong> --&gt; <strong>The first to employ memory module on video anomaly detection</strong><br> [4] ğŸ‘Object-Centric Auto-Encoders and Dummy Anomalies for Abnormal Event Detection <strong>(CVPR 2019)</strong> --&gt; <strong>The first to combine object detection and vad to achieve object-level anomaly dtection.</strong><br> [5] AnoPCN: Video Anomaly Detection via Deep Predictive Coding Network <strong>(ACM MM 2019)</strong> --&gt; <strong>The first hybrid model</strong><br> [6] ğŸ‘Learning Memory-guided Normality for Anomaly Detection <strong>(CVPR 2020)</strong> --&gt; <strong>Based on MemAE</strong><br> [7] Cluster Attention Contrast for Video Anomaly Detection <strong>(ACM MM 2020)</strong> --&gt; <strong>The first to apply Contrastive Learninig</strong><br> [8] ğŸ‘Anomaly Detection in Video via Self-Supervised and Multi-Task Learning <strong>(CVPR 2021)</strong> --&gt; <strong>object-level</strong><br> [9] ğŸ‘A Hybrid Video Anomaly Detection Framework via Memory-Augmented Flow Reconstruction and Flow-Guided Frame Prediction <strong>(ICCV 2021)</strong> --&gt; <strong>Hybrid model</strong><br> [10] Anomaly Detection in Video Sequence with Appearance-Motion Correspondence (ICCV 2019) --&gt; <strong>Two stream network</strong><br> [11] Video Anomaly Detection and Localization via Gaussian Mixture Fully Convolutional Variational Autoencoder --&gt; <strong>Two stream network</strong><br> [12] Self-supervised Sparse Representation for Video Anomaly Detection <strong>(ECCV 2022)</strong> --&gt; A first attempt to slove unsupervised and weakly supervised VAD<br> [13] Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles <strong>(ECCV 2022)</strong></p></li><li><p><strong>Joural Papers</strong><br> [1] VideoÂ AnomalyÂ DetectionÂ with Sparse Coding Inspired Deep Neural Networks <strong>(TPAMI 2021)</strong><br> [2] A Background-Agnostic Framework With Adversarial Training for Abnormal EventÂ DetectionÂ inÂ Video <strong>(TPAMI 2022)</strong><br> [3] Influence-Aware Attention Networks forÂ AnomalyÂ DetectionÂ in SurveillanceÂ Videos <strong>(TCSVT 2022)</strong><br> [4] Bidirectional Spatio-Temporal Feature Learning With Multiscale Evaluation for Video Anomaly Detection <strong>(TCSVT 2022)</strong><br> [5] AnomalyÂ DetectionÂ With Bidirectional Consistency inÂ Videos <strong>(TNNLS 2022)</strong><br> [6] Variational Abnormal BehaviorÂ DetectionÂ With Motion Consistency <strong>(TIP 2022)</strong><br> [7] DoTA: Unsupervised Detection of Traffic Anomaly in Driving Videos <strong>(TPAMI 2023)</strong><br> [8] A Hierarchical Spatio-Temporal Graph Convolutional Neural Network forÂ AnomalyÂ DetectionÂ inÂ Videos <strong>(TCSVT 2023)</strong><br> [9] Learnable Locality-Sensitive Hashing forÂ VideoÂ AnomalyÂ Detection <strong>(TCSVT 2023)</strong><br> [10] A Kalman Variational Autoencoder Model Assisted by Odometric Clustering forÂ VideoÂ Frame Prediction andÂ AnomalyÂ Detection <strong>(TIP 2023)</strong><br> [11] Abnormal EventÂ DetectionÂ and Localization via Adversarial Event Prediction <strong>(TNNLS 2023)</strong></p></li></ul><h6 id="â– -weakly-supervised-vad" tabindex="-1"><a class="header-anchor" href="#â– -weakly-supervised-vad"><span>â–  Weakly supervised VAD</span></a></h6><p>[1] ğŸ‘ Real-world Anomaly Detection in Surveillance Videos <strong>(CVPR 2018)</strong><br> [2] Weakly Supervised Video Anomaly Detection via Center-Guided Discrimative Learning <strong>(ICME 2020)</strong></p><p>[3] Decouple and Resolve: Transformer-Based Models for OnlineÂ AnomalyÂ DetectionÂ From Weakly LabeledÂ Videos <strong>(TIFS 2023)</strong></p><h5 id="â¢-ç»å…¸é¡¹ç›®-1" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸é¡¹ç›®-1"><span>â¢ ç»å…¸é¡¹ç›®</span></a></h5><p>â—‹ MNAD --&gt; <a href="https://github.com/cvlab-yonsei/MNAD" target="_blank" rel="noopener noreferrer">https://github.com/cvlab-yonsei/MNAD</a> å¯ä½œä¸ºbaseline.</p><h5 id="â¢-å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘-explainable-anomaly-detection-ead-å¯è§£é‡Šæ€§å¼‚å¸¸æ£€æµ‹" tabindex="-1"><a class="header-anchor" href="#â¢-å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘-explainable-anomaly-detection-ead-å¯è§£é‡Šæ€§å¼‚å¸¸æ£€æµ‹"><span>â¢ å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘--&gt; Explainable Anomaly Detection (EAD) å¯è§£é‡Šæ€§å¼‚å¸¸æ£€æµ‹</span></a></h5><h6 id="_1-definition" tabindex="-1"><a class="header-anchor" href="#_1-definition"><span>1. DEFINITION</span></a></h6><p>The aim of this TASK is to detect and automatically generate high-level explanations of anomalous events in video. Understanding the cause of an anomalous event is crucialas the required response is dependant on its nature andseverity. --&gt; Anomaly Detection &amp; Anoamly Explanation</p><h6 id="_2-related-work" tabindex="-1"><a class="header-anchor" href="#_2-related-work"><span>2. RELATED WORK</span></a></h6><p>[1] Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge (ICCV 2017)<br> [2] X-MAN: Explaining multiple sources of anomalies in video (CVPR workshop 2021)<br> [3] Discrete neural representations for explainable anomaly detection (WACV 2022)</p><h3 id="âœ§-èˆªæ‹å›¾åƒç›®æ ‡æ£€æµ‹-drone-view-object-detection-è«æ¢¦ç«Ÿæˆ" tabindex="-1"><a class="header-anchor" href="#âœ§-èˆªæ‹å›¾åƒç›®æ ‡æ£€æµ‹-drone-view-object-detection-è«æ¢¦ç«Ÿæˆ"><span>âœ§ èˆªæ‹å›¾åƒç›®æ ‡æ£€æµ‹ï¼ˆDrone-view Object Detectionï¼‰-&gt; <code>è«æ¢¦ç«Ÿæˆ</code></span></a></h3>',26)),a("æ¨¡ç‰ˆ"),e[15]||(e[15]=r('<h4 id="è®ºæ–‡æ±‡æ€»-è®ºæ–‡æ±‡æ€»é“¾æ¥-ä¾‹å¦‚githubä¸Šçš„-awesome-ç³»åˆ—-2" tabindex="-1"><a class="header-anchor" href="#è®ºæ–‡æ±‡æ€»-è®ºæ–‡æ±‡æ€»é“¾æ¥-ä¾‹å¦‚githubä¸Šçš„-awesome-ç³»åˆ—-2"><span>è®ºæ–‡æ±‡æ€»ï¼šè®ºæ–‡æ±‡æ€»é“¾æ¥ï¼Œä¾‹å¦‚GitHubä¸Šçš„ <code>Awesome</code> ç³»åˆ—</span></a></h4><h5 id="â¢-æ¨èç»¼è¿°-3" tabindex="-1"><a class="header-anchor" href="#â¢-æ¨èç»¼è¿°-3"><span>â¢ æ¨èç»¼è¿°</span></a></h5><p>[1] ã€å‘è¡¨äºã€‘ã€å¹´ä»½ã€‘ã€è®ºæ–‡åã€‘ã€å›¢é˜Ÿåã€‘ã€è®ºæ–‡é“¾æ¥ã€‘ã€ä»£ç é“¾æ¥ã€‘ã€ç®€ä»‹ã€‘<br> [2] ã€çŸ¥ä¹/å¾®ä¿¡/åšå®¢ç­‰ç½‘é¡µé“¾æ¥ã€‘ã€å†…å®¹ç®€ä»‹ã€‘</p><h5 id="â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-4" tabindex="-1"><a class="header-anchor" href="#â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-4"><span>â¢ ä¼˜ç§€å›¢é˜Ÿ / å­¦æœ¯å¤§ä½¬</span></a></h5><p>â–  ã€å›¢é˜Ÿåã€‘ã€å›¢é˜Ÿé“¾æ¥ã€‘ã€æ‰€å±æœºæ„ã€‘ã€ç»†åŒ–æ–¹å‘ã€‘<br> [1] ã€ä»£è¡¨è®ºæ–‡ã€‘<br> â–  ã€å¤§ä½¬åã€‘ã€ä¸ªäººä¸»é¡µã€‘ã€æ‰€å±æœºæ„ã€‘ã€ç»†åŒ–æ–¹å‘ã€‘<br> [1] ã€ä»£è¡¨è®ºæ–‡ã€‘</p><h5 id="â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘-2" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘-2"><span>â¢ ç»å…¸è®ºæ–‡ï¼šï¼ˆæ¨èåŠ â€œğŸ‘â€ï¼‰</span></a></h5><p>[1] ã€å‘è¡¨äºã€‘ã€å¹´ä»½ã€‘ã€è®ºæ–‡åã€‘ã€å›¢é˜Ÿåã€‘ã€è®ºæ–‡é“¾æ¥ã€‘ã€ä»£ç é“¾æ¥ã€‘ã€ç®€ä»‹ã€‘<br> [2] ğŸ‘ã€å‘è¡¨äºã€‘ã€å¹´ä»½ã€‘ã€è®ºæ–‡åã€‘ã€å›¢é˜Ÿåã€‘ã€è®ºæ–‡é“¾æ¥ã€‘ã€ä»£ç é“¾æ¥ã€‘ã€ç®€ä»‹ã€‘</p><h5 id="â¢-ç»å…¸é¡¹ç›®-2" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸é¡¹ç›®-2"><span>â¢ ç»å…¸é¡¹ç›®</span></a></h5><p>â—‹ ã€åç§°ã€‘ã€é“¾æ¥ã€‘ã€ç®€ä»‹ã€‘</p><h3 id="âœ§-å°æ ·æœ¬ç›®æ ‡æ£€æµ‹-few-shot-object-detection-é™ˆæ³°å²³" tabindex="-1"><a class="header-anchor" href="#âœ§-å°æ ·æœ¬ç›®æ ‡æ£€æµ‹-few-shot-object-detection-é™ˆæ³°å²³"><span>âœ§ å°æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFew-shot Object Detectionï¼‰-&gt; <code>é™ˆæ³°å²³</code></span></a></h3>',10)),a("æ¨¡ç‰ˆ"),e[16]||(e[16]=r('<h4 id="è®ºæ–‡æ±‡æ€»-è®ºæ–‡æ±‡æ€»é“¾æ¥-ä¾‹å¦‚githubä¸Šçš„-awesome-ç³»åˆ—-3" tabindex="-1"><a class="header-anchor" href="#è®ºæ–‡æ±‡æ€»-è®ºæ–‡æ±‡æ€»é“¾æ¥-ä¾‹å¦‚githubä¸Šçš„-awesome-ç³»åˆ—-3"><span>è®ºæ–‡æ±‡æ€»ï¼šè®ºæ–‡æ±‡æ€»é“¾æ¥ï¼Œä¾‹å¦‚GitHubä¸Šçš„ <code>Awesome</code> ç³»åˆ—</span></a></h4><h5 id="â¢-æ¨èç»¼è¿°-4" tabindex="-1"><a class="header-anchor" href="#â¢-æ¨èç»¼è¿°-4"><span>â¢ æ¨èç»¼è¿°</span></a></h5><p>[1] ã€å‘è¡¨äºã€‘ã€å¹´ä»½ã€‘ã€è®ºæ–‡åã€‘ã€å›¢é˜Ÿåã€‘ã€è®ºæ–‡é“¾æ¥ã€‘ã€ä»£ç é“¾æ¥ã€‘ã€ç®€ä»‹ã€‘<br> [2] ã€çŸ¥ä¹/å¾®ä¿¡/åšå®¢ç­‰ç½‘é¡µé“¾æ¥ã€‘ã€å†…å®¹ç®€ä»‹ã€‘</p><h5 id="â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-5" tabindex="-1"><a class="header-anchor" href="#â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-5"><span>â¢ ä¼˜ç§€å›¢é˜Ÿ / å­¦æœ¯å¤§ä½¬</span></a></h5><p>â–  ã€å›¢é˜Ÿåã€‘ã€å›¢é˜Ÿé“¾æ¥ã€‘ã€æ‰€å±æœºæ„ã€‘ã€ç»†åŒ–æ–¹å‘ã€‘<br> [1] ã€ä»£è¡¨è®ºæ–‡ã€‘<br> â–  ã€å¤§ä½¬åã€‘ã€ä¸ªäººä¸»é¡µã€‘ã€æ‰€å±æœºæ„ã€‘ã€ç»†åŒ–æ–¹å‘ã€‘<br> [1] ã€ä»£è¡¨è®ºæ–‡ã€‘</p><h5 id="â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘-3" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘-3"><span>â¢ ç»å…¸è®ºæ–‡ï¼šï¼ˆæ¨èåŠ â€œğŸ‘â€ï¼‰</span></a></h5><p>[1] ã€å‘è¡¨äºã€‘ã€å¹´ä»½ã€‘ã€è®ºæ–‡åã€‘ã€å›¢é˜Ÿåã€‘ã€è®ºæ–‡é“¾æ¥ã€‘ã€ä»£ç é“¾æ¥ã€‘ã€ç®€ä»‹ã€‘<br> [2] ğŸ‘ã€å‘è¡¨äºã€‘ã€å¹´ä»½ã€‘ã€è®ºæ–‡åã€‘ã€å›¢é˜Ÿåã€‘ã€è®ºæ–‡é“¾æ¥ã€‘ã€ä»£ç é“¾æ¥ã€‘ã€ç®€ä»‹ã€‘</p><h5 id="â¢-ç»å…¸é¡¹ç›®-3" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸é¡¹ç›®-3"><span>â¢ ç»å…¸é¡¹ç›®</span></a></h5><p>â—‹ ã€åç§°ã€‘ã€é“¾æ¥ã€‘ã€ç®€ä»‹ã€‘</p><blockquote><p>å‚è€ƒæ–‡æ¡£ï¼š</p><ol><li>ã€åœ¨iiplabåšç§‘ç ”ã€‘ <a href="https://docs.qq.com/pdf/DR3NNa2xqU0Rld1B2" target="_blank" rel="noopener noreferrer">https://docs.qq.com/pdf/DR3NNa2xqU0Rld1B2</a>ï¼Œç‰¹åˆ«æ„Ÿè°¢ï½</li></ol></blockquote>',10))])}const f=l(c,[["render",g]]),w=JSON.parse(`{"path":"/browser/learning/README%20(2).html","title":"ReadMeğŸ§","lang":"en-US","frontmatter":{"description":"ReadMeğŸ§ ä»‹ç» è¯¥æ–‡æ¡£é¡¹ç›®ä¸º Happy Learning å°ç»„çš„ç§‘ç ”å…¥é—¨èµ„æ–™ã€‚ ReadMeä»…åŒ…å«åŸºç¡€å†…å®¹ï¼Œå¯ä½œä¸ºç´¢å¼•ä½¿ç”¨ï¼Œè¯¦ç»†å†…å®¹è¯·è§å¯¹åº”æ–‡æ¡£ã€‚ é¡¹ç›®åœ°å€: http://10.16.104.13:1805/happy-learning/tohappylearning å°ç»„è€å¤§ å†·ä½³æ—­ https://faculty.cqupt.edu.c...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"ReadMeğŸ§\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-17T15:06:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Hongyi Wang\\",\\"url\\":\\"https://wanghy1997.github.io/wanghy1997\\"}]}"],["meta",{"property":"og:url","content":"https://wanghy1997.github.io/wanghy1997/browser/learning/README%20(2).html"}],["meta",{"property":"og:site_name","content":"Hongyi's Blog"}],["meta",{"property":"og:title","content":"ReadMeğŸ§"}],["meta",{"property":"og:description","content":"ReadMeğŸ§ ä»‹ç» è¯¥æ–‡æ¡£é¡¹ç›®ä¸º Happy Learning å°ç»„çš„ç§‘ç ”å…¥é—¨èµ„æ–™ã€‚ ReadMeä»…åŒ…å«åŸºç¡€å†…å®¹ï¼Œå¯ä½œä¸ºç´¢å¼•ä½¿ç”¨ï¼Œè¯¦ç»†å†…å®¹è¯·è§å¯¹åº”æ–‡æ¡£ã€‚ é¡¹ç›®åœ°å€: http://10.16.104.13:1805/happy-learning/tohappylearning å°ç»„è€å¤§ å†·ä½³æ—­ https://faculty.cqupt.edu.c..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-09-17T15:06:21.000Z"}],["meta",{"property":"article:modified_time","content":"2025-09-17T15:06:21.000Z"}]]},"git":{"createdTime":1758121581000,"updatedTime":1758121581000,"contributors":[{"name":"whymbp","username":"whymbp","email":"why_6267@163.com","commits":1,"url":"https://github.com/whymbp"}]},"readingTime":{"minutes":14.53,"words":4360},"filePathRelative":"browser/learning/README (2).md","excerpt":"\\n<h2>ä»‹ç»</h2>\\n<p>è¯¥æ–‡æ¡£é¡¹ç›®ä¸º Happy Learning å°ç»„çš„ç§‘ç ”å…¥é—¨èµ„æ–™ã€‚<br>\\nReadMeä»…åŒ…å«åŸºç¡€å†…å®¹ï¼Œå¯ä½œä¸ºç´¢å¼•ä½¿ç”¨ï¼Œè¯¦ç»†å†…å®¹è¯·è§å¯¹åº”æ–‡æ¡£ã€‚<br>\\né¡¹ç›®åœ°å€: <a href=\\"http://10.16.104.13:1805/happy-learning/tohappylearning\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">http://10.16.104.13:1805/happy-learning/tohappylearning</a></p>\\n<h2>å°ç»„è€å¤§</h2>\\n<p>å†·ä½³æ—­  <a href=\\"https://faculty.cqupt.edu.cn/lengjiaxu/zh_CN/index.htm\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://faculty.cqupt.edu.cn/lengjiaxu/zh_CN/index.htm</a></p>","autoDesc":true}`);export{f as comp,w as data};
