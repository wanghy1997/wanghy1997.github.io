import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,b as n,o as a}from"./app-B8nmrpbQ.js";const o={};function i(p,e){return a(),r("div",null,[...e[0]||(e[0]=[n('<p><a href="https://github.com/PJLab-ADG/awesome-knowledge-driven-AD" target="_blank" rel="noopener noreferrer">https://github.com/PJLab-ADG/awesome-knowledge-driven-AD</a></p><p>CVPR2024 论文接收列表 <a href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers" target="_blank" rel="noopener noreferrer">https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers</a></p><p>LocLLM: Exploiting Generalizable Human Keypoint Localization<br> via Large Language Model<br><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_LocLLM_Exploiting_Generalizable_Human_Keypoint_Localization_via_Large_Language_Model_CVPR_2024_paper.pdf" target="_blank" rel="noopener noreferrer">https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_LocLLM_Exploiting_Generalizable_Human_Keypoint_Localization_via_Large_Language_Model_CVPR_2024_paper.pdf</a></p><p><a href="https://github.com/kennethwdk/LocLLM" target="_blank" rel="noopener noreferrer">https://github.com/kennethwdk/LocLLM</a></p><p>PointLLM: Empowering Large Language Models to Understand Point Clouds<br><a href="https://github.com/OpenRobotLab/PointLLM?tab=readme-ov-file" target="_blank" rel="noopener noreferrer">https://github.com/OpenRobotLab/PointLLM?tab=readme-ov-file</a></p><p>HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion<br> Models (CVPR&#39;24)<br><a href="https://handdiffuse.github.io/" target="_blank" rel="noopener noreferrer">https://handdiffuse.github.io/</a></p><p>GPT4Point : A Unified Framework for Point-Language Understanding and Generation<br><a href="https://github.com/Pointcept/GPT4Point" target="_blank" rel="noopener noreferrer">https://github.com/Pointcept/GPT4Point</a></p><p>Visual In-Context Prompting<br><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Visual_In-Context_Prompting_CVPR_2024_paper.pdf" target="_blank" rel="noopener noreferrer">https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Visual_In-Context_Prompting_CVPR_2024_paper.pdf</a></p><p>Visual Instruction Tuning<br><a href="https://llava-vl.github.io" target="_blank" rel="noopener noreferrer">https://llava-vl.github.io</a></p><p>Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs<br><a href="https://arxiv.org/pdf/2401.06209" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2401.06209</a><br><a href="https://github.com/tsb0601/MMVP" target="_blank" rel="noopener noreferrer">https://github.com/tsb0601/MMVP</a></p><h2 id="describing-differences-in-image-sets-with-natural-language" tabindex="-1"><a class="header-anchor" href="#describing-differences-in-image-sets-with-natural-language"><span>Describing Differences in Image Sets with Natural Language</span></a></h2><p><a href="https://understanding-visual-datasets.github.io/VisDiff-website/" target="_blank" rel="noopener noreferrer">https://understanding-visual-datasets.github.io/VisDiff-website/</a><br><a href="https://github.com/Understanding-Visual-Datasets/VisDiff" target="_blank" rel="noopener noreferrer">https://github.com/Understanding-Visual-Datasets/VisDiff</a><br><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Dunlap_Describing_Differences_in_Image_Sets_with_Natural_Language_CVPR_2024_paper.pdf" target="_blank" rel="noopener noreferrer">https://openaccess.thecvf.com/content/CVPR2024/papers/Dunlap_Describing_Differences_in_Image_Sets_with_Natural_Language_CVPR_2024_paper.pdf</a><br> blog: <a href="https://voxel51.com/blog/cvpr-2024-survival-guide-five-vision-language-papers-you-dont-want-to-miss/" target="_blank" rel="noopener noreferrer">https://voxel51.com/blog/cvpr-2024-survival-guide-five-vision-language-papers-you-dont-want-to-miss/</a></p><p>Low-Resource Vision Challenges for Foundation Models<br><a href="https://xiaobai1217.github.io/Low-Resource-Vision/" target="_blank" rel="noopener noreferrer">https://xiaobai1217.github.io/Low-Resource-Vision/</a></p>',13)])])}const g=t(o,[["render",i]]),h=JSON.parse(`{"path":"/discover/uncover/survey.html","title":"","lang":"en-US","frontmatter":{"description":"https://github.com/PJLab-ADG/awesome-knowledge-driven-AD CVPR2024 论文接收列表 https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers LocLLM: Exploiting Generalizable Human Keypoint ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-17T15:06:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Hongyi Wang\\",\\"url\\":\\"https://wanghy1997.github.io/wanghy1997\\"}]}"],["meta",{"property":"og:url","content":"https://wanghy1997.github.io/wanghy1997/discover/uncover/survey.html"}],["meta",{"property":"og:site_name","content":"Hongyi's Blog"}],["meta",{"property":"og:description","content":"https://github.com/PJLab-ADG/awesome-knowledge-driven-AD CVPR2024 论文接收列表 https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers LocLLM: Exploiting Generalizable Human Keypoint ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-09-17T15:06:21.000Z"}],["meta",{"property":"article:modified_time","content":"2025-09-17T15:06:21.000Z"}]]},"git":{"createdTime":1758121581000,"updatedTime":1758121581000,"contributors":[{"name":"whymbp","username":"whymbp","email":"why_6267@163.com","commits":1,"url":"https://github.com/whymbp"}]},"readingTime":{"minutes":0.47,"words":142},"filePathRelative":"discover/uncover/survey.md","excerpt":"<p><a href=\\"https://github.com/PJLab-ADG/awesome-knowledge-driven-AD\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://github.com/PJLab-ADG/awesome-knowledge-driven-AD</a></p>\\n<p>CVPR2024 论文接收列表 <a href=\\"https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers</a></p>","autoDesc":true}`);export{g as comp,h as data};
