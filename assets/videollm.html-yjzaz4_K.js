import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as o,b as a,o as n}from"./app-B-wDcIMq.js";const r={};function i(d,e){return n(),o("div",null,[...e[0]||(e[0]=[a('<p>Awesome-Multimodal-Large-Language-Models<br><a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models" target="_blank" rel="noopener noreferrer">https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</a></p><p>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</p><p><a href="https://github.com/DAMO-NLP-SG/VideoLLaMA2" target="_blank" rel="noopener noreferrer">https://github.com/DAMO-NLP-SG/VideoLLaMA2</a></p><p>AskVideos-VideoCLIP<br><a href="https://github.com/AskYoutubeAI/AskVideos-VideoCLIP" target="_blank" rel="noopener noreferrer">https://github.com/AskYoutubeAI/AskVideos-VideoCLIP</a></p><p>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding<br><a href="https://github.com/OpenGVLab/InternVideo" target="_blank" rel="noopener noreferrer">https://github.com/OpenGVLab/InternVideo</a></p><p>INTERNVIDEO2: SCALING VIDEO FOUNDATION MODELS FOR MULTIMODAL VIDEO UNDERSTANDING<br><a href="https://github.com/OpenGVLab/InternVideo2" target="_blank" rel="noopener noreferrer">https://github.com/OpenGVLab/InternVideo2</a></p><p>InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation<br><a href="https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid" target="_blank" rel="noopener noreferrer">https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid</a></p><p>ViCLIP: a video-text representation learning model trained on InternVid<br><a href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo1/Pretrain/ViCLIP" target="_blank" rel="noopener noreferrer">https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo1/Pretrain/ViCLIP</a></p>',8)])])}const g=t(r,[["render",i]]),m=JSON.parse(`{"path":"/discover/uncover/videollm.html","title":"","lang":"en-US","frontmatter":{"description":"Awesome-Multimodal-Large-Language-Models https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understa...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-17T15:06:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Hongyi Wang\\",\\"url\\":\\"https://wanghy1997.github.io/wanghy1997\\"}]}"],["meta",{"property":"og:url","content":"https://wanghy1997.github.io/wanghy1997/discover/uncover/videollm.html"}],["meta",{"property":"og:site_name","content":"Hongyi's Blog"}],["meta",{"property":"og:description","content":"Awesome-Multimodal-Large-Language-Models https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understa..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-09-17T15:06:21.000Z"}],["meta",{"property":"article:modified_time","content":"2025-09-17T15:06:21.000Z"}]]},"git":{"createdTime":1758121581000,"updatedTime":1758121581000,"contributors":[{"name":"whymbp","username":"whymbp","email":"why_6267@163.com","commits":1,"url":"https://github.com/whymbp"}]},"readingTime":{"minutes":0.27,"words":80},"filePathRelative":"discover/uncover/videollm.md","excerpt":"<p>Awesome-Multimodal-Large-Language-Models<br>\\n<a href=\\"https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</a></p>\\n<p>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</p>","autoDesc":true}`);export{g as comp,m as data};
